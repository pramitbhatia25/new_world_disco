{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25839e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\socia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "\n",
    "filepath = './tess_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8141ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the default kepler data loader in exonet.py\n",
    "# reference: https://gitlab.com/frontierdevelopmentlab/exoplanets/exonet-pytorch/-/blob/master/exonet.py\n",
    "class KeplerDataLoader(Dataset):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    PURPOSE: DATA LOADER FOR KERPLER LIGHT CURVES\n",
    "    INPUT: PATH TO DIRECTOR WITH LIGHT CURVES + INFO FILES\n",
    "    OUTPUT: LOCAL + GLOBAL VIEWS, LABELS\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "\n",
    "        ### list of global, local, and info files (assumes certain names of files)\n",
    "        self.flist_global = np.sort(glob.glob(os.path.join(filepath, '*global_flux.npy')))\n",
    "        self.flist_local = np.sort(glob.glob(os.path.join(filepath, '*local_flux.npy')))\n",
    "        self.flist_info = np.sort(glob.glob(os.path.join(filepath, '*info.npy')))\n",
    "        \n",
    "        print(len(self.flist_global))\n",
    "        \n",
    "        ### list of whitened centroid files\n",
    "        self.flist_global_cen = np.sort(glob.glob(os.path.join(filepath, '*global_cen.npy')))\n",
    "        self.flist_local_cen = np.sort(glob.glob(os.path.join(filepath, '*local_cen.npy')))\n",
    "        \n",
    "        ### ids = {TIC}_{TCE}\n",
    "        self.ids = np.sort([(x.split('/')[-1]).split('_')[1] + '_' + (x.split('/')[-1]).split('_')[2] for x in self.flist_global])\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "\n",
    "        ### grab local and global views\n",
    "        data_global = np.load(self.flist_global[idx]).astype(np.float32)\n",
    "        data_local = np.load(self.flist_local[idx]).astype(np.float32)\n",
    "\n",
    "        ### grab centroid views\n",
    "        data_global_cen = np.load(self.flist_global_cen[idx]).astype(np.float32)\n",
    "        data_local_cen = np.load(self.flist_local_cen[idx]).astype(np.float32)\n",
    "        \n",
    "        ### info file contains: [0]kic, [1]tce, [2]period, [3]epoch, [4]duration, [5]label)\n",
    "        data_info = np.load(self.flist_info[idx])\n",
    "\n",
    "        return (data_local, data_global, data_local_cen, data_global_cen, data_info[3:]), data_info[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574a4a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n"
     ]
    }
   ],
   "source": [
    "tess_data = KeplerDataLoader(filepath = filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6973c099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data): 2\n",
      "len(data[0]): 5\n",
      "shape data[0][0]: (data_local): (61,)\n",
      "shape data[0][1]: (data_global): (201,)\n",
      "shape data[0][2]: (data_local_cen): (61,)\n",
      "shape data[0][3]: (data_global_cen): (201,)\n",
      "shape data[0][4]: (data_info[6:]): (17,)\n",
      "data[1]: 1.0\n",
      "\n",
      "len(data): 2\n",
      "len(data[0]): 5\n",
      "shape data[0][0]: (data_local): (61,)\n",
      "shape data[0][1]: (data_global): (201,)\n",
      "shape data[0][2]: (data_local_cen): (61,)\n",
      "shape data[0][3]: (data_global_cen): (201,)\n",
      "shape data[0][4]: (data_info[6:]): (17,)\n",
      "data[1]: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look through some of the data\n",
    "count = 0\n",
    "\n",
    "for data in tess_data:\n",
    "    print(\"len(data):\", len(data))\n",
    "    print(\"len(data[0]):\", len(data[0]))\n",
    "    print(\"shape data[0][0]: (data_local):\", data[0][0].shape)\n",
    "    print(\"shape data[0][1]: (data_global):\", data[0][1].shape)\n",
    "    print(\"shape data[0][2]: (data_local_cen):\", data[0][2].shape)\n",
    "    print(\"shape data[0][3]: (data_global_cen):\", data[0][3].shape)\n",
    "    print(\"shape data[0][4]: (data_info[6:]):\", data[0][4].shape)\n",
    "    print(\"data[1]:\", data[1])\n",
    "    print()\n",
    "\n",
    "    count += 1\n",
    "    if count == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b907f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data:\n",
      "0 / 3060\n",
      "1 / 3060\n",
      "2 / 3060\n",
      "3 / 3060\n",
      "4 / 3060\n",
      "5 / 3060\n",
      "6 / 3060\n",
      "7 / 3060\n",
      "8 / 3060\n",
      "9 / 3060\n",
      "10 / 3060\n",
      "11 / 3060\n",
      "12 / 3060\n",
      "13 / 3060\n",
      "14 / 3060\n",
      "15 / 3060\n",
      "16 / 3060\n",
      "17 / 3060\n",
      "18 / 3060\n",
      "19 / 3060\n",
      "20 / 3060\n",
      "21 / 3060\n",
      "22 / 3060\n",
      "23 / 3060\n",
      "24 / 3060\n",
      "25 / 3060\n",
      "26 / 3060\n",
      "27 / 3060\n",
      "28 / 3060\n",
      "29 / 3060\n",
      "30 / 3060\n",
      "31 / 3060\n",
      "32 / 3060\n",
      "33 / 3060\n",
      "34 / 3060\n",
      "35 / 3060\n",
      "36 / 3060\n",
      "37 / 3060\n",
      "38 / 3060\n",
      "39 / 3060\n",
      "40 / 3060\n",
      "41 / 3060\n",
      "42 / 3060\n",
      "43 / 3060\n",
      "44 / 3060\n",
      "45 / 3060\n",
      "46 / 3060\n",
      "47 / 3060\n",
      "48 / 3060\n",
      "49 / 3060\n",
      "50 / 3060\n",
      "51 / 3060\n",
      "52 / 3060\n",
      "53 / 3060\n",
      "54 / 3060\n",
      "55 / 3060\n",
      "56 / 3060\n",
      "57 / 3060\n",
      "58 / 3060\n",
      "59 / 3060\n",
      "60 / 3060\n",
      "61 / 3060\n",
      "62 / 3060\n",
      "63 / 3060\n",
      "64 / 3060\n",
      "65 / 3060\n",
      "66 / 3060\n",
      "67 / 3060\n",
      "68 / 3060\n",
      "69 / 3060\n",
      "70 / 3060\n",
      "71 / 3060\n",
      "72 / 3060\n",
      "73 / 3060\n",
      "74 / 3060\n",
      "75 / 3060\n",
      "76 / 3060\n",
      "77 / 3060\n",
      "78 / 3060\n",
      "79 / 3060\n",
      "80 / 3060\n",
      "81 / 3060\n",
      "82 / 3060\n",
      "83 / 3060\n",
      "84 / 3060\n",
      "85 / 3060\n",
      "86 / 3060\n",
      "87 / 3060\n",
      "88 / 3060\n",
      "89 / 3060\n",
      "90 / 3060\n",
      "91 / 3060\n",
      "92 / 3060\n",
      "93 / 3060\n",
      "94 / 3060\n",
      "95 / 3060\n",
      "96 / 3060\n",
      "97 / 3060\n",
      "98 / 3060\n",
      "99 / 3060\n",
      "100 / 3060\n",
      "101 / 3060\n",
      "102 / 3060\n",
      "103 / 3060\n",
      "104 / 3060\n",
      "105 / 3060\n",
      "106 / 3060\n",
      "107 / 3060\n",
      "108 / 3060\n",
      "109 / 3060\n",
      "110 / 3060\n",
      "111 / 3060\n",
      "112 / 3060\n",
      "113 / 3060\n",
      "114 / 3060\n",
      "115 / 3060\n",
      "116 / 3060\n",
      "117 / 3060\n",
      "118 / 3060\n",
      "119 / 3060\n",
      "120 / 3060\n",
      "121 / 3060\n",
      "122 / 3060\n",
      "123 / 3060\n",
      "124 / 3060\n",
      "125 / 3060\n",
      "126 / 3060\n",
      "127 / 3060\n",
      "128 / 3060\n",
      "129 / 3060\n",
      "130 / 3060\n",
      "131 / 3060\n",
      "132 / 3060\n",
      "133 / 3060\n",
      "134 / 3060\n",
      "135 / 3060\n",
      "136 / 3060\n",
      "137 / 3060\n",
      "138 / 3060\n",
      "139 / 3060\n",
      "140 / 3060\n",
      "141 / 3060\n",
      "142 / 3060\n",
      "143 / 3060\n",
      "144 / 3060\n",
      "145 / 3060\n",
      "146 / 3060\n",
      "147 / 3060\n",
      "148 / 3060\n",
      "149 / 3060\n",
      "150 / 3060\n",
      "151 / 3060\n",
      "152 / 3060\n",
      "153 / 3060\n",
      "154 / 3060\n",
      "155 / 3060\n",
      "156 / 3060\n",
      "157 / 3060\n",
      "158 / 3060\n",
      "159 / 3060\n",
      "160 / 3060\n",
      "161 / 3060\n",
      "162 / 3060\n",
      "163 / 3060\n",
      "164 / 3060\n",
      "165 / 3060\n",
      "166 / 3060\n",
      "167 / 3060\n",
      "168 / 3060\n",
      "169 / 3060\n",
      "170 / 3060\n",
      "171 / 3060\n",
      "172 / 3060\n",
      "173 / 3060\n",
      "174 / 3060\n",
      "175 / 3060\n",
      "176 / 3060\n",
      "177 / 3060\n",
      "178 / 3060\n",
      "179 / 3060\n",
      "180 / 3060\n",
      "181 / 3060\n",
      "182 / 3060\n",
      "183 / 3060\n",
      "184 / 3060\n",
      "185 / 3060\n",
      "186 / 3060\n",
      "187 / 3060\n",
      "188 / 3060\n",
      "189 / 3060\n",
      "190 / 3060\n",
      "191 / 3060\n",
      "192 / 3060\n",
      "193 / 3060\n",
      "194 / 3060\n",
      "195 / 3060\n",
      "196 / 3060\n",
      "197 / 3060\n",
      "198 / 3060\n",
      "199 / 3060\n",
      "200 / 3060\n",
      "201 / 3060\n",
      "202 / 3060\n",
      "203 / 3060\n",
      "204 / 3060\n",
      "205 / 3060\n",
      "206 / 3060\n",
      "207 / 3060\n",
      "208 / 3060\n",
      "209 / 3060\n",
      "210 / 3060\n",
      "211 / 3060\n",
      "212 / 3060\n",
      "213 / 3060\n",
      "214 / 3060\n",
      "215 / 3060\n",
      "216 / 3060\n",
      "217 / 3060\n",
      "218 / 3060\n",
      "219 / 3060\n",
      "220 / 3060\n",
      "221 / 3060\n",
      "222 / 3060\n",
      "223 / 3060\n",
      "224 / 3060\n",
      "225 / 3060\n",
      "226 / 3060\n",
      "227 / 3060\n",
      "228 / 3060\n",
      "229 / 3060\n",
      "230 / 3060\n",
      "231 / 3060\n",
      "232 / 3060\n",
      "233 / 3060\n",
      "234 / 3060\n",
      "235 / 3060\n",
      "236 / 3060\n",
      "237 / 3060\n",
      "238 / 3060\n",
      "239 / 3060\n",
      "240 / 3060\n",
      "241 / 3060\n",
      "242 / 3060\n",
      "243 / 3060\n",
      "244 / 3060\n",
      "245 / 3060\n",
      "246 / 3060\n",
      "247 / 3060\n",
      "248 / 3060\n",
      "249 / 3060\n",
      "250 / 3060\n",
      "251 / 3060\n",
      "252 / 3060\n",
      "253 / 3060\n",
      "254 / 3060\n",
      "255 / 3060\n",
      "256 / 3060\n",
      "257 / 3060\n",
      "258 / 3060\n",
      "259 / 3060\n",
      "260 / 3060\n",
      "261 / 3060\n",
      "262 / 3060\n",
      "263 / 3060\n",
      "264 / 3060\n",
      "265 / 3060\n",
      "266 / 3060\n",
      "267 / 3060\n",
      "268 / 3060\n",
      "269 / 3060\n",
      "270 / 3060\n",
      "271 / 3060\n",
      "272 / 3060\n",
      "273 / 3060\n",
      "274 / 3060\n",
      "275 / 3060\n",
      "276 / 3060\n",
      "277 / 3060\n",
      "278 / 3060\n",
      "279 / 3060\n",
      "280 / 3060\n",
      "281 / 3060\n",
      "282 / 3060\n",
      "283 / 3060\n",
      "284 / 3060\n",
      "285 / 3060\n",
      "286 / 3060\n",
      "287 / 3060\n",
      "288 / 3060\n",
      "289 / 3060\n",
      "290 / 3060\n",
      "291 / 3060\n",
      "292 / 3060\n",
      "293 / 3060\n",
      "294 / 3060\n",
      "295 / 3060\n",
      "296 / 3060\n",
      "297 / 3060\n",
      "298 / 3060\n",
      "299 / 3060\n",
      "300 / 3060\n",
      "301 / 3060\n",
      "302 / 3060\n",
      "303 / 3060\n",
      "304 / 3060\n",
      "305 / 3060\n",
      "306 / 3060\n",
      "307 / 3060\n",
      "308 / 3060\n",
      "309 / 3060\n",
      "310 / 3060\n",
      "311 / 3060\n",
      "312 / 3060\n",
      "313 / 3060\n",
      "314 / 3060\n",
      "315 / 3060\n",
      "316 / 3060\n",
      "317 / 3060\n",
      "318 / 3060\n",
      "319 / 3060\n",
      "320 / 3060\n",
      "321 / 3060\n",
      "322 / 3060\n",
      "323 / 3060\n",
      "324 / 3060\n",
      "325 / 3060\n",
      "326 / 3060\n",
      "327 / 3060\n",
      "328 / 3060\n",
      "329 / 3060\n",
      "330 / 3060\n",
      "331 / 3060\n",
      "332 / 3060\n",
      "333 / 3060\n",
      "334 / 3060\n",
      "335 / 3060\n",
      "336 / 3060\n",
      "337 / 3060\n",
      "338 / 3060\n",
      "339 / 3060\n",
      "340 / 3060\n",
      "341 / 3060\n",
      "342 / 3060\n",
      "343 / 3060\n",
      "344 / 3060\n",
      "345 / 3060\n",
      "346 / 3060\n",
      "347 / 3060\n",
      "348 / 3060\n",
      "349 / 3060\n",
      "350 / 3060\n",
      "351 / 3060\n",
      "352 / 3060\n",
      "353 / 3060\n",
      "354 / 3060\n",
      "355 / 3060\n",
      "356 / 3060\n",
      "357 / 3060\n",
      "358 / 3060\n",
      "359 / 3060\n",
      "360 / 3060\n",
      "361 / 3060\n",
      "362 / 3060\n",
      "363 / 3060\n",
      "364 / 3060\n",
      "365 / 3060\n",
      "366 / 3060\n",
      "367 / 3060\n",
      "368 / 3060\n",
      "369 / 3060\n",
      "370 / 3060\n",
      "371 / 3060\n",
      "372 / 3060\n",
      "373 / 3060\n",
      "374 / 3060\n",
      "375 / 3060\n",
      "376 / 3060\n",
      "377 / 3060\n",
      "378 / 3060\n",
      "379 / 3060\n",
      "380 / 3060\n",
      "381 / 3060\n",
      "382 / 3060\n",
      "383 / 3060\n",
      "384 / 3060\n",
      "385 / 3060\n",
      "386 / 3060\n",
      "387 / 3060\n",
      "388 / 3060\n",
      "389 / 3060\n",
      "390 / 3060\n",
      "391 / 3060\n",
      "392 / 3060\n",
      "393 / 3060\n",
      "394 / 3060\n",
      "395 / 3060\n",
      "396 / 3060\n",
      "397 / 3060\n",
      "398 / 3060\n",
      "399 / 3060\n",
      "400 / 3060\n",
      "401 / 3060\n",
      "402 / 3060\n",
      "403 / 3060\n",
      "404 / 3060\n",
      "405 / 3060\n",
      "406 / 3060\n",
      "407 / 3060\n",
      "408 / 3060\n",
      "409 / 3060\n",
      "410 / 3060\n",
      "411 / 3060\n",
      "412 / 3060\n",
      "413 / 3060\n",
      "414 / 3060\n",
      "415 / 3060\n",
      "416 / 3060\n",
      "417 / 3060\n",
      "418 / 3060\n",
      "419 / 3060\n",
      "420 / 3060\n",
      "421 / 3060\n",
      "422 / 3060\n",
      "423 / 3060\n",
      "424 / 3060\n",
      "425 / 3060\n",
      "426 / 3060\n",
      "427 / 3060\n",
      "428 / 3060\n",
      "429 / 3060\n",
      "430 / 3060\n",
      "431 / 3060\n",
      "432 / 3060\n",
      "433 / 3060\n",
      "434 / 3060\n",
      "435 / 3060\n",
      "436 / 3060\n",
      "437 / 3060\n",
      "438 / 3060\n",
      "439 / 3060\n",
      "440 / 3060\n",
      "441 / 3060\n",
      "442 / 3060\n",
      "443 / 3060\n",
      "444 / 3060\n",
      "445 / 3060\n",
      "446 / 3060\n",
      "447 / 3060\n",
      "448 / 3060\n",
      "449 / 3060\n",
      "450 / 3060\n",
      "451 / 3060\n",
      "452 / 3060\n",
      "453 / 3060\n",
      "454 / 3060\n",
      "455 / 3060\n",
      "456 / 3060\n",
      "457 / 3060\n",
      "458 / 3060\n",
      "459 / 3060\n",
      "460 / 3060\n",
      "461 / 3060\n",
      "462 / 3060\n",
      "463 / 3060\n",
      "464 / 3060\n",
      "465 / 3060\n",
      "466 / 3060\n",
      "467 / 3060\n",
      "468 / 3060\n",
      "469 / 3060\n",
      "470 / 3060\n",
      "471 / 3060\n",
      "472 / 3060\n",
      "473 / 3060\n",
      "474 / 3060\n",
      "475 / 3060\n",
      "476 / 3060\n",
      "477 / 3060\n",
      "478 / 3060\n",
      "479 / 3060\n",
      "480 / 3060\n",
      "481 / 3060\n",
      "482 / 3060\n",
      "483 / 3060\n",
      "484 / 3060\n",
      "485 / 3060\n",
      "486 / 3060\n",
      "487 / 3060\n",
      "488 / 3060\n",
      "489 / 3060\n",
      "490 / 3060\n",
      "491 / 3060\n",
      "492 / 3060\n",
      "493 / 3060\n",
      "494 / 3060\n",
      "495 / 3060\n",
      "496 / 3060\n",
      "497 / 3060\n",
      "498 / 3060\n",
      "499 / 3060\n",
      "500 / 3060\n",
      "501 / 3060\n",
      "502 / 3060\n",
      "503 / 3060\n",
      "504 / 3060\n",
      "505 / 3060\n",
      "506 / 3060\n",
      "507 / 3060\n",
      "508 / 3060\n",
      "509 / 3060\n",
      "510 / 3060\n",
      "511 / 3060\n",
      "512 / 3060\n",
      "513 / 3060\n",
      "514 / 3060\n",
      "515 / 3060\n",
      "516 / 3060\n",
      "517 / 3060\n",
      "518 / 3060\n",
      "519 / 3060\n",
      "520 / 3060\n",
      "521 / 3060\n",
      "522 / 3060\n",
      "523 / 3060\n",
      "524 / 3060\n",
      "525 / 3060\n",
      "526 / 3060\n",
      "527 / 3060\n",
      "528 / 3060\n",
      "529 / 3060\n",
      "530 / 3060\n",
      "531 / 3060\n",
      "532 / 3060\n",
      "533 / 3060\n",
      "534 / 3060\n",
      "535 / 3060\n",
      "536 / 3060\n",
      "537 / 3060\n",
      "538 / 3060\n",
      "539 / 3060\n",
      "540 / 3060\n",
      "541 / 3060\n",
      "542 / 3060\n",
      "543 / 3060\n",
      "544 / 3060\n",
      "545 / 3060\n",
      "546 / 3060\n",
      "547 / 3060\n",
      "548 / 3060\n",
      "549 / 3060\n",
      "550 / 3060\n",
      "551 / 3060\n",
      "552 / 3060\n",
      "553 / 3060\n",
      "554 / 3060\n",
      "555 / 3060\n",
      "556 / 3060\n",
      "557 / 3060\n",
      "558 / 3060\n",
      "559 / 3060\n",
      "560 / 3060\n",
      "561 / 3060\n",
      "562 / 3060\n",
      "563 / 3060\n",
      "564 / 3060\n",
      "565 / 3060\n",
      "566 / 3060\n",
      "567 / 3060\n",
      "568 / 3060\n",
      "569 / 3060\n",
      "570 / 3060\n",
      "571 / 3060\n",
      "572 / 3060\n",
      "573 / 3060\n",
      "574 / 3060\n",
      "575 / 3060\n",
      "576 / 3060\n",
      "577 / 3060\n",
      "578 / 3060\n",
      "579 / 3060\n",
      "580 / 3060\n",
      "581 / 3060\n",
      "582 / 3060\n",
      "583 / 3060\n",
      "584 / 3060\n",
      "585 / 3060\n",
      "586 / 3060\n",
      "587 / 3060\n",
      "588 / 3060\n",
      "589 / 3060\n",
      "590 / 3060\n",
      "591 / 3060\n",
      "592 / 3060\n",
      "593 / 3060\n",
      "594 / 3060\n",
      "595 / 3060\n",
      "596 / 3060\n",
      "597 / 3060\n",
      "598 / 3060\n",
      "599 / 3060\n",
      "600 / 3060\n",
      "601 / 3060\n",
      "602 / 3060\n",
      "603 / 3060\n",
      "604 / 3060\n",
      "605 / 3060\n",
      "606 / 3060\n",
      "607 / 3060\n",
      "608 / 3060\n",
      "609 / 3060\n",
      "610 / 3060\n",
      "611 / 3060\n",
      "612 / 3060\n",
      "613 / 3060\n",
      "614 / 3060\n",
      "615 / 3060\n",
      "616 / 3060\n",
      "617 / 3060\n",
      "618 / 3060\n",
      "619 / 3060\n",
      "620 / 3060\n",
      "621 / 3060\n",
      "622 / 3060\n",
      "623 / 3060\n",
      "624 / 3060\n",
      "625 / 3060\n",
      "626 / 3060\n",
      "627 / 3060\n",
      "628 / 3060\n",
      "629 / 3060\n",
      "630 / 3060\n",
      "631 / 3060\n",
      "632 / 3060\n",
      "633 / 3060\n",
      "634 / 3060\n",
      "635 / 3060\n",
      "636 / 3060\n",
      "637 / 3060\n",
      "638 / 3060\n",
      "639 / 3060\n",
      "640 / 3060\n",
      "641 / 3060\n",
      "642 / 3060\n",
      "643 / 3060\n",
      "644 / 3060\n",
      "645 / 3060\n",
      "646 / 3060\n",
      "647 / 3060\n",
      "648 / 3060\n",
      "649 / 3060\n",
      "650 / 3060\n",
      "651 / 3060\n",
      "652 / 3060\n",
      "653 / 3060\n",
      "654 / 3060\n",
      "655 / 3060\n",
      "656 / 3060\n",
      "657 / 3060\n",
      "658 / 3060\n",
      "659 / 3060\n",
      "660 / 3060\n",
      "661 / 3060\n",
      "662 / 3060\n",
      "663 / 3060\n",
      "664 / 3060\n",
      "665 / 3060\n",
      "666 / 3060\n",
      "667 / 3060\n",
      "668 / 3060\n",
      "669 / 3060\n",
      "670 / 3060\n",
      "671 / 3060\n",
      "672 / 3060\n",
      "673 / 3060\n",
      "674 / 3060\n",
      "675 / 3060\n",
      "676 / 3060\n",
      "677 / 3060\n",
      "678 / 3060\n",
      "679 / 3060\n",
      "680 / 3060\n",
      "681 / 3060\n",
      "682 / 3060\n",
      "683 / 3060\n",
      "684 / 3060\n",
      "685 / 3060\n",
      "686 / 3060\n",
      "687 / 3060\n",
      "688 / 3060\n",
      "689 / 3060\n",
      "690 / 3060\n",
      "691 / 3060\n",
      "692 / 3060\n",
      "693 / 3060\n",
      "694 / 3060\n",
      "695 / 3060\n",
      "696 / 3060\n",
      "697 / 3060\n",
      "698 / 3060\n",
      "699 / 3060\n",
      "700 / 3060\n",
      "701 / 3060\n",
      "702 / 3060\n",
      "703 / 3060\n",
      "704 / 3060\n",
      "705 / 3060\n",
      "706 / 3060\n",
      "707 / 3060\n",
      "708 / 3060\n",
      "709 / 3060\n",
      "710 / 3060\n",
      "711 / 3060\n",
      "712 / 3060\n",
      "713 / 3060\n",
      "714 / 3060\n",
      "715 / 3060\n",
      "716 / 3060\n",
      "717 / 3060\n",
      "718 / 3060\n",
      "719 / 3060\n",
      "720 / 3060\n",
      "721 / 3060\n",
      "722 / 3060\n",
      "723 / 3060\n",
      "724 / 3060\n",
      "725 / 3060\n",
      "726 / 3060\n",
      "727 / 3060\n",
      "728 / 3060\n",
      "729 / 3060\n",
      "730 / 3060\n",
      "731 / 3060\n",
      "732 / 3060\n",
      "733 / 3060\n",
      "734 / 3060\n",
      "735 / 3060\n",
      "736 / 3060\n",
      "737 / 3060\n",
      "738 / 3060\n",
      "739 / 3060\n",
      "740 / 3060\n",
      "741 / 3060\n",
      "742 / 3060\n",
      "743 / 3060\n",
      "744 / 3060\n",
      "745 / 3060\n",
      "746 / 3060\n",
      "747 / 3060\n",
      "748 / 3060\n",
      "749 / 3060\n",
      "750 / 3060\n",
      "751 / 3060\n",
      "752 / 3060\n",
      "753 / 3060\n",
      "754 / 3060\n",
      "755 / 3060\n",
      "756 / 3060\n",
      "757 / 3060\n",
      "758 / 3060\n",
      "759 / 3060\n",
      "760 / 3060\n",
      "761 / 3060\n",
      "762 / 3060\n",
      "763 / 3060\n",
      "764 / 3060\n",
      "765 / 3060\n",
      "766 / 3060\n",
      "767 / 3060\n",
      "768 / 3060\n",
      "769 / 3060\n",
      "770 / 3060\n",
      "771 / 3060\n",
      "772 / 3060\n",
      "773 / 3060\n",
      "774 / 3060\n",
      "775 / 3060\n",
      "776 / 3060\n",
      "777 / 3060\n",
      "778 / 3060\n",
      "779 / 3060\n",
      "780 / 3060\n",
      "781 / 3060\n",
      "782 / 3060\n",
      "783 / 3060\n",
      "784 / 3060\n",
      "785 / 3060\n",
      "786 / 3060\n",
      "787 / 3060\n",
      "788 / 3060\n",
      "789 / 3060\n",
      "790 / 3060\n",
      "791 / 3060\n",
      "792 / 3060\n",
      "793 / 3060\n",
      "794 / 3060\n",
      "795 / 3060\n",
      "796 / 3060\n",
      "797 / 3060\n",
      "798 / 3060\n",
      "799 / 3060\n",
      "800 / 3060\n",
      "801 / 3060\n",
      "802 / 3060\n",
      "803 / 3060\n",
      "804 / 3060\n",
      "805 / 3060\n",
      "806 / 3060\n",
      "807 / 3060\n",
      "808 / 3060\n",
      "809 / 3060\n",
      "810 / 3060\n",
      "811 / 3060\n",
      "812 / 3060\n",
      "813 / 3060\n",
      "814 / 3060\n",
      "815 / 3060\n",
      "816 / 3060\n",
      "817 / 3060\n",
      "818 / 3060\n",
      "819 / 3060\n",
      "820 / 3060\n",
      "821 / 3060\n",
      "822 / 3060\n",
      "823 / 3060\n",
      "824 / 3060\n",
      "825 / 3060\n",
      "826 / 3060\n",
      "827 / 3060\n",
      "828 / 3060\n",
      "829 / 3060\n",
      "830 / 3060\n",
      "831 / 3060\n",
      "832 / 3060\n",
      "833 / 3060\n",
      "834 / 3060\n",
      "835 / 3060\n",
      "836 / 3060\n",
      "837 / 3060\n",
      "838 / 3060\n",
      "839 / 3060\n",
      "840 / 3060\n",
      "841 / 3060\n",
      "842 / 3060\n",
      "843 / 3060\n",
      "844 / 3060\n",
      "845 / 3060\n",
      "846 / 3060\n",
      "847 / 3060\n",
      "848 / 3060\n",
      "849 / 3060\n",
      "850 / 3060\n",
      "851 / 3060\n",
      "852 / 3060\n",
      "853 / 3060\n",
      "854 / 3060\n",
      "855 / 3060\n",
      "856 / 3060\n",
      "857 / 3060\n",
      "858 / 3060\n",
      "859 / 3060\n",
      "860 / 3060\n",
      "861 / 3060\n",
      "862 / 3060\n",
      "863 / 3060\n",
      "864 / 3060\n",
      "865 / 3060\n",
      "866 / 3060\n",
      "867 / 3060\n",
      "868 / 3060\n",
      "869 / 3060\n",
      "870 / 3060\n",
      "871 / 3060\n",
      "872 / 3060\n",
      "873 / 3060\n",
      "874 / 3060\n",
      "875 / 3060\n",
      "876 / 3060\n",
      "877 / 3060\n",
      "878 / 3060\n",
      "879 / 3060\n",
      "880 / 3060\n",
      "881 / 3060\n",
      "882 / 3060\n",
      "883 / 3060\n",
      "884 / 3060\n",
      "885 / 3060\n",
      "886 / 3060\n",
      "887 / 3060\n",
      "888 / 3060\n",
      "889 / 3060\n",
      "890 / 3060\n",
      "891 / 3060\n",
      "892 / 3060\n",
      "893 / 3060\n",
      "894 / 3060\n",
      "895 / 3060\n",
      "896 / 3060\n",
      "897 / 3060\n",
      "898 / 3060\n",
      "899 / 3060\n",
      "900 / 3060\n",
      "901 / 3060\n",
      "902 / 3060\n",
      "903 / 3060\n",
      "904 / 3060\n",
      "905 / 3060\n",
      "906 / 3060\n",
      "907 / 3060\n",
      "908 / 3060\n",
      "909 / 3060\n",
      "910 / 3060\n",
      "911 / 3060\n",
      "912 / 3060\n",
      "913 / 3060\n",
      "914 / 3060\n",
      "915 / 3060\n",
      "916 / 3060\n",
      "917 / 3060\n",
      "918 / 3060\n",
      "919 / 3060\n",
      "920 / 3060\n",
      "921 / 3060\n",
      "922 / 3060\n",
      "923 / 3060\n",
      "924 / 3060\n",
      "925 / 3060\n",
      "926 / 3060\n",
      "927 / 3060\n",
      "928 / 3060\n",
      "929 / 3060\n",
      "930 / 3060\n",
      "931 / 3060\n",
      "932 / 3060\n",
      "933 / 3060\n",
      "934 / 3060\n",
      "935 / 3060\n",
      "936 / 3060\n",
      "937 / 3060\n",
      "938 / 3060\n",
      "939 / 3060\n",
      "940 / 3060\n",
      "941 / 3060\n",
      "942 / 3060\n",
      "943 / 3060\n",
      "944 / 3060\n",
      "945 / 3060\n",
      "946 / 3060\n",
      "947 / 3060\n",
      "948 / 3060\n",
      "949 / 3060\n",
      "950 / 3060\n",
      "951 / 3060\n",
      "952 / 3060\n",
      "953 / 3060\n",
      "954 / 3060\n",
      "955 / 3060\n",
      "956 / 3060\n",
      "957 / 3060\n",
      "958 / 3060\n",
      "959 / 3060\n",
      "960 / 3060\n",
      "961 / 3060\n",
      "962 / 3060\n",
      "963 / 3060\n",
      "964 / 3060\n",
      "965 / 3060\n",
      "966 / 3060\n",
      "967 / 3060\n",
      "968 / 3060\n",
      "969 / 3060\n",
      "970 / 3060\n",
      "971 / 3060\n",
      "972 / 3060\n",
      "973 / 3060\n",
      "974 / 3060\n",
      "975 / 3060\n",
      "976 / 3060\n",
      "977 / 3060\n",
      "978 / 3060\n",
      "979 / 3060\n",
      "980 / 3060\n",
      "981 / 3060\n",
      "982 / 3060\n",
      "983 / 3060\n",
      "984 / 3060\n",
      "985 / 3060\n",
      "986 / 3060\n",
      "987 / 3060\n",
      "988 / 3060\n",
      "989 / 3060\n",
      "990 / 3060\n",
      "991 / 3060\n",
      "992 / 3060\n",
      "993 / 3060\n",
      "994 / 3060\n",
      "995 / 3060\n",
      "996 / 3060\n",
      "997 / 3060\n",
      "998 / 3060\n",
      "999 / 3060\n",
      "1000 / 3060\n",
      "1001 / 3060\n",
      "1002 / 3060\n",
      "1003 / 3060\n",
      "1004 / 3060\n",
      "1005 / 3060\n",
      "1006 / 3060\n",
      "1007 / 3060\n",
      "1008 / 3060\n",
      "1009 / 3060\n",
      "1010 / 3060\n",
      "1011 / 3060\n",
      "1012 / 3060\n",
      "1013 / 3060\n",
      "1014 / 3060\n",
      "1015 / 3060\n",
      "1016 / 3060\n",
      "1017 / 3060\n",
      "1018 / 3060\n",
      "1019 / 3060\n",
      "1020 / 3060\n",
      "1021 / 3060\n",
      "1022 / 3060\n",
      "1023 / 3060\n",
      "1024 / 3060\n",
      "1025 / 3060\n",
      "1026 / 3060\n",
      "1027 / 3060\n",
      "1028 / 3060\n",
      "1029 / 3060\n",
      "1030 / 3060\n",
      "1031 / 3060\n",
      "1032 / 3060\n",
      "1033 / 3060\n",
      "1034 / 3060\n",
      "1035 / 3060\n",
      "1036 / 3060\n",
      "1037 / 3060\n",
      "1038 / 3060\n",
      "1039 / 3060\n",
      "1040 / 3060\n",
      "1041 / 3060\n",
      "1042 / 3060\n",
      "1043 / 3060\n",
      "1044 / 3060\n",
      "1045 / 3060\n",
      "1046 / 3060\n",
      "1047 / 3060\n",
      "1048 / 3060\n",
      "1049 / 3060\n",
      "1050 / 3060\n",
      "1051 / 3060\n",
      "1052 / 3060\n",
      "1053 / 3060\n",
      "1054 / 3060\n",
      "1055 / 3060\n",
      "1056 / 3060\n",
      "1057 / 3060\n",
      "1058 / 3060\n",
      "1059 / 3060\n",
      "1060 / 3060\n",
      "1061 / 3060\n",
      "1062 / 3060\n",
      "1063 / 3060\n",
      "1064 / 3060\n",
      "1065 / 3060\n",
      "1066 / 3060\n",
      "1067 / 3060\n",
      "1068 / 3060\n",
      "1069 / 3060\n",
      "1070 / 3060\n",
      "1071 / 3060\n",
      "1072 / 3060\n",
      "1073 / 3060\n",
      "1074 / 3060\n",
      "1075 / 3060\n",
      "1076 / 3060\n",
      "1077 / 3060\n",
      "1078 / 3060\n",
      "1079 / 3060\n",
      "1080 / 3060\n",
      "1081 / 3060\n",
      "1082 / 3060\n",
      "1083 / 3060\n",
      "1084 / 3060\n",
      "1085 / 3060\n",
      "1086 / 3060\n",
      "1087 / 3060\n",
      "1088 / 3060\n",
      "1089 / 3060\n",
      "1090 / 3060\n",
      "1091 / 3060\n",
      "1092 / 3060\n",
      "1093 / 3060\n",
      "1094 / 3060\n",
      "1095 / 3060\n",
      "1096 / 3060\n",
      "1097 / 3060\n",
      "1098 / 3060\n",
      "1099 / 3060\n",
      "1100 / 3060\n",
      "1101 / 3060\n",
      "1102 / 3060\n",
      "1103 / 3060\n",
      "1104 / 3060\n",
      "1105 / 3060\n",
      "1106 / 3060\n",
      "1107 / 3060\n",
      "1108 / 3060\n",
      "1109 / 3060\n",
      "1110 / 3060\n",
      "1111 / 3060\n",
      "1112 / 3060\n",
      "1113 / 3060\n",
      "1114 / 3060\n",
      "1115 / 3060\n",
      "1116 / 3060\n",
      "1117 / 3060\n",
      "1118 / 3060\n",
      "1119 / 3060\n",
      "1120 / 3060\n",
      "1121 / 3060\n",
      "1122 / 3060\n",
      "1123 / 3060\n",
      "1124 / 3060\n",
      "1125 / 3060\n",
      "1126 / 3060\n",
      "1127 / 3060\n",
      "1128 / 3060\n",
      "1129 / 3060\n",
      "1130 / 3060\n",
      "1131 / 3060\n",
      "1132 / 3060\n",
      "1133 / 3060\n",
      "1134 / 3060\n",
      "1135 / 3060\n",
      "1136 / 3060\n",
      "1137 / 3060\n",
      "1138 / 3060\n",
      "1139 / 3060\n",
      "1140 / 3060\n",
      "1141 / 3060\n",
      "1142 / 3060\n",
      "1143 / 3060\n",
      "1144 / 3060\n",
      "1145 / 3060\n",
      "1146 / 3060\n",
      "1147 / 3060\n",
      "1148 / 3060\n",
      "1149 / 3060\n",
      "1150 / 3060\n",
      "1151 / 3060\n",
      "1152 / 3060\n",
      "1153 / 3060\n",
      "1154 / 3060\n",
      "1155 / 3060\n",
      "1156 / 3060\n",
      "1157 / 3060\n",
      "1158 / 3060\n",
      "1159 / 3060\n",
      "1160 / 3060\n",
      "1161 / 3060\n",
      "1162 / 3060\n",
      "1163 / 3060\n",
      "1164 / 3060\n",
      "1165 / 3060\n",
      "1166 / 3060\n",
      "1167 / 3060\n",
      "1168 / 3060\n",
      "1169 / 3060\n",
      "1170 / 3060\n",
      "1171 / 3060\n",
      "1172 / 3060\n",
      "1173 / 3060\n",
      "1174 / 3060\n",
      "1175 / 3060\n",
      "1176 / 3060\n",
      "1177 / 3060\n",
      "1178 / 3060\n",
      "1179 / 3060\n",
      "1180 / 3060\n",
      "1181 / 3060\n",
      "1182 / 3060\n",
      "1183 / 3060\n",
      "1184 / 3060\n",
      "1185 / 3060\n",
      "1186 / 3060\n",
      "1187 / 3060\n",
      "1188 / 3060\n",
      "1189 / 3060\n",
      "1190 / 3060\n",
      "1191 / 3060\n",
      "1192 / 3060\n",
      "1193 / 3060\n",
      "1194 / 3060\n",
      "1195 / 3060\n",
      "1196 / 3060\n",
      "1197 / 3060\n",
      "1198 / 3060\n",
      "1199 / 3060\n",
      "1200 / 3060\n",
      "1201 / 3060\n",
      "1202 / 3060\n",
      "1203 / 3060\n",
      "1204 / 3060\n",
      "1205 / 3060\n",
      "1206 / 3060\n",
      "1207 / 3060\n",
      "1208 / 3060\n",
      "1209 / 3060\n",
      "1210 / 3060\n",
      "1211 / 3060\n",
      "1212 / 3060\n",
      "1213 / 3060\n",
      "1214 / 3060\n",
      "1215 / 3060\n",
      "1216 / 3060\n",
      "1217 / 3060\n",
      "1218 / 3060\n",
      "1219 / 3060\n",
      "1220 / 3060\n",
      "1221 / 3060\n",
      "1222 / 3060\n",
      "1223 / 3060\n",
      "1224 / 3060\n",
      "1225 / 3060\n",
      "1226 / 3060\n",
      "1227 / 3060\n",
      "1228 / 3060\n",
      "1229 / 3060\n",
      "1230 / 3060\n",
      "1231 / 3060\n",
      "1232 / 3060\n",
      "1233 / 3060\n",
      "1234 / 3060\n",
      "1235 / 3060\n",
      "1236 / 3060\n",
      "1237 / 3060\n",
      "1238 / 3060\n",
      "1239 / 3060\n",
      "1240 / 3060\n",
      "1241 / 3060\n",
      "1242 / 3060\n",
      "1243 / 3060\n",
      "1244 / 3060\n",
      "1245 / 3060\n",
      "1246 / 3060\n",
      "1247 / 3060\n",
      "1248 / 3060\n",
      "1249 / 3060\n",
      "1250 / 3060\n",
      "1251 / 3060\n",
      "1252 / 3060\n",
      "1253 / 3060\n",
      "1254 / 3060\n",
      "1255 / 3060\n",
      "1256 / 3060\n",
      "1257 / 3060\n",
      "1258 / 3060\n",
      "1259 / 3060\n",
      "1260 / 3060\n",
      "1261 / 3060\n",
      "1262 / 3060\n",
      "1263 / 3060\n",
      "1264 / 3060\n",
      "1265 / 3060\n",
      "1266 / 3060\n",
      "1267 / 3060\n",
      "1268 / 3060\n",
      "1269 / 3060\n",
      "1270 / 3060\n",
      "1271 / 3060\n",
      "1272 / 3060\n",
      "1273 / 3060\n",
      "1274 / 3060\n",
      "1275 / 3060\n",
      "1276 / 3060\n",
      "1277 / 3060\n",
      "1278 / 3060\n",
      "1279 / 3060\n",
      "1280 / 3060\n",
      "1281 / 3060\n",
      "1282 / 3060\n",
      "1283 / 3060\n",
      "1284 / 3060\n",
      "1285 / 3060\n",
      "1286 / 3060\n",
      "1287 / 3060\n",
      "1288 / 3060\n",
      "1289 / 3060\n",
      "1290 / 3060\n",
      "1291 / 3060\n",
      "1292 / 3060\n",
      "1293 / 3060\n",
      "1294 / 3060\n",
      "1295 / 3060\n",
      "1296 / 3060\n",
      "1297 / 3060\n",
      "1298 / 3060\n",
      "1299 / 3060\n",
      "1300 / 3060\n",
      "1301 / 3060\n",
      "1302 / 3060\n",
      "1303 / 3060\n",
      "1304 / 3060\n",
      "1305 / 3060\n",
      "1306 / 3060\n",
      "1307 / 3060\n",
      "1308 / 3060\n",
      "1309 / 3060\n",
      "1310 / 3060\n",
      "1311 / 3060\n",
      "1312 / 3060\n",
      "1313 / 3060\n",
      "1314 / 3060\n",
      "1315 / 3060\n",
      "1316 / 3060\n",
      "1317 / 3060\n",
      "1318 / 3060\n",
      "1319 / 3060\n",
      "1320 / 3060\n",
      "1321 / 3060\n",
      "1322 / 3060\n",
      "1323 / 3060\n",
      "1324 / 3060\n",
      "1325 / 3060\n",
      "1326 / 3060\n",
      "1327 / 3060\n",
      "1328 / 3060\n",
      "1329 / 3060\n",
      "1330 / 3060\n",
      "1331 / 3060\n",
      "1332 / 3060\n",
      "1333 / 3060\n",
      "1334 / 3060\n",
      "1335 / 3060\n",
      "1336 / 3060\n",
      "1337 / 3060\n",
      "1338 / 3060\n",
      "1339 / 3060\n",
      "1340 / 3060\n",
      "1341 / 3060\n",
      "1342 / 3060\n",
      "1343 / 3060\n",
      "1344 / 3060\n",
      "1345 / 3060\n",
      "1346 / 3060\n",
      "1347 / 3060\n",
      "1348 / 3060\n",
      "1349 / 3060\n",
      "1350 / 3060\n",
      "1351 / 3060\n",
      "1352 / 3060\n",
      "1353 / 3060\n",
      "1354 / 3060\n",
      "1355 / 3060\n",
      "1356 / 3060\n",
      "1357 / 3060\n",
      "1358 / 3060\n",
      "1359 / 3060\n",
      "1360 / 3060\n",
      "1361 / 3060\n",
      "1362 / 3060\n",
      "1363 / 3060\n",
      "1364 / 3060\n",
      "1365 / 3060\n",
      "1366 / 3060\n",
      "1367 / 3060\n",
      "1368 / 3060\n",
      "1369 / 3060\n",
      "1370 / 3060\n",
      "1371 / 3060\n",
      "1372 / 3060\n",
      "1373 / 3060\n",
      "1374 / 3060\n",
      "1375 / 3060\n",
      "1376 / 3060\n",
      "1377 / 3060\n",
      "1378 / 3060\n",
      "1379 / 3060\n",
      "1380 / 3060\n",
      "1381 / 3060\n",
      "1382 / 3060\n",
      "1383 / 3060\n",
      "1384 / 3060\n",
      "1385 / 3060\n",
      "1386 / 3060\n",
      "1387 / 3060\n",
      "1388 / 3060\n",
      "1389 / 3060\n",
      "1390 / 3060\n",
      "1391 / 3060\n",
      "1392 / 3060\n",
      "1393 / 3060\n",
      "1394 / 3060\n",
      "1395 / 3060\n",
      "1396 / 3060\n",
      "1397 / 3060\n",
      "1398 / 3060\n",
      "1399 / 3060\n",
      "1400 / 3060\n",
      "1401 / 3060\n",
      "1402 / 3060\n",
      "1403 / 3060\n",
      "1404 / 3060\n",
      "1405 / 3060\n",
      "1406 / 3060\n",
      "1407 / 3060\n",
      "1408 / 3060\n",
      "1409 / 3060\n",
      "1410 / 3060\n",
      "1411 / 3060\n",
      "1412 / 3060\n",
      "1413 / 3060\n",
      "1414 / 3060\n",
      "1415 / 3060\n",
      "1416 / 3060\n",
      "1417 / 3060\n",
      "1418 / 3060\n",
      "1419 / 3060\n",
      "1420 / 3060\n",
      "1421 / 3060\n",
      "1422 / 3060\n",
      "1423 / 3060\n",
      "1424 / 3060\n",
      "1425 / 3060\n",
      "1426 / 3060\n",
      "1427 / 3060\n",
      "1428 / 3060\n",
      "1429 / 3060\n",
      "1430 / 3060\n",
      "1431 / 3060\n",
      "1432 / 3060\n",
      "1433 / 3060\n",
      "1434 / 3060\n",
      "1435 / 3060\n",
      "1436 / 3060\n",
      "1437 / 3060\n",
      "1438 / 3060\n",
      "1439 / 3060\n",
      "1440 / 3060\n",
      "1441 / 3060\n",
      "1442 / 3060\n",
      "1443 / 3060\n",
      "1444 / 3060\n",
      "1445 / 3060\n",
      "1446 / 3060\n",
      "1447 / 3060\n",
      "1448 / 3060\n",
      "1449 / 3060\n",
      "1450 / 3060\n",
      "1451 / 3060\n",
      "1452 / 3060\n",
      "1453 / 3060\n",
      "1454 / 3060\n",
      "1455 / 3060\n",
      "1456 / 3060\n",
      "1457 / 3060\n",
      "1458 / 3060\n",
      "1459 / 3060\n",
      "1460 / 3060\n",
      "1461 / 3060\n",
      "1462 / 3060\n",
      "1463 / 3060\n",
      "1464 / 3060\n",
      "1465 / 3060\n",
      "1466 / 3060\n",
      "1467 / 3060\n",
      "1468 / 3060\n",
      "1469 / 3060\n",
      "1470 / 3060\n",
      "1471 / 3060\n",
      "1472 / 3060\n",
      "1473 / 3060\n",
      "1474 / 3060\n",
      "1475 / 3060\n",
      "1476 / 3060\n",
      "1477 / 3060\n",
      "1478 / 3060\n",
      "1479 / 3060\n",
      "1480 / 3060\n",
      "1481 / 3060\n",
      "1482 / 3060\n",
      "1483 / 3060\n",
      "1484 / 3060\n",
      "1485 / 3060\n",
      "1486 / 3060\n",
      "1487 / 3060\n",
      "1488 / 3060\n",
      "1489 / 3060\n",
      "1490 / 3060\n",
      "1491 / 3060\n",
      "1492 / 3060\n",
      "1493 / 3060\n",
      "1494 / 3060\n",
      "1495 / 3060\n",
      "1496 / 3060\n",
      "1497 / 3060\n",
      "1498 / 3060\n",
      "1499 / 3060\n",
      "1500 / 3060\n",
      "1501 / 3060\n",
      "1502 / 3060\n",
      "1503 / 3060\n",
      "1504 / 3060\n",
      "1505 / 3060\n",
      "1506 / 3060\n",
      "1507 / 3060\n",
      "1508 / 3060\n",
      "1509 / 3060\n",
      "1510 / 3060\n",
      "1511 / 3060\n",
      "1512 / 3060\n",
      "1513 / 3060\n",
      "1514 / 3060\n",
      "1515 / 3060\n",
      "1516 / 3060\n",
      "1517 / 3060\n",
      "1518 / 3060\n",
      "1519 / 3060\n",
      "1520 / 3060\n",
      "1521 / 3060\n",
      "1522 / 3060\n",
      "1523 / 3060\n",
      "1524 / 3060\n",
      "1525 / 3060\n",
      "1526 / 3060\n",
      "1527 / 3060\n",
      "1528 / 3060\n",
      "1529 / 3060\n",
      "1530 / 3060\n",
      "1531 / 3060\n",
      "1532 / 3060\n",
      "1533 / 3060\n",
      "1534 / 3060\n",
      "1535 / 3060\n",
      "1536 / 3060\n",
      "1537 / 3060\n",
      "1538 / 3060\n",
      "1539 / 3060\n",
      "1540 / 3060\n",
      "1541 / 3060\n",
      "1542 / 3060\n",
      "1543 / 3060\n",
      "1544 / 3060\n",
      "1545 / 3060\n",
      "1546 / 3060\n",
      "1547 / 3060\n",
      "1548 / 3060\n",
      "1549 / 3060\n",
      "1550 / 3060\n",
      "1551 / 3060\n",
      "1552 / 3060\n",
      "1553 / 3060\n",
      "1554 / 3060\n",
      "1555 / 3060\n",
      "1556 / 3060\n",
      "1557 / 3060\n",
      "1558 / 3060\n",
      "1559 / 3060\n",
      "1560 / 3060\n",
      "1561 / 3060\n",
      "1562 / 3060\n",
      "1563 / 3060\n",
      "1564 / 3060\n",
      "1565 / 3060\n",
      "1566 / 3060\n",
      "1567 / 3060\n",
      "1568 / 3060\n",
      "1569 / 3060\n",
      "1570 / 3060\n",
      "1571 / 3060\n",
      "1572 / 3060\n",
      "1573 / 3060\n",
      "1574 / 3060\n",
      "1575 / 3060\n",
      "1576 / 3060\n",
      "1577 / 3060\n",
      "1578 / 3060\n",
      "1579 / 3060\n",
      "1580 / 3060\n",
      "1581 / 3060\n",
      "1582 / 3060\n",
      "1583 / 3060\n",
      "1584 / 3060\n",
      "1585 / 3060\n",
      "1586 / 3060\n",
      "1587 / 3060\n",
      "1588 / 3060\n",
      "1589 / 3060\n",
      "1590 / 3060\n",
      "1591 / 3060\n",
      "1592 / 3060\n",
      "1593 / 3060\n",
      "1594 / 3060\n",
      "1595 / 3060\n",
      "1596 / 3060\n",
      "1597 / 3060\n",
      "1598 / 3060\n",
      "1599 / 3060\n",
      "1600 / 3060\n",
      "1601 / 3060\n",
      "1602 / 3060\n",
      "1603 / 3060\n",
      "1604 / 3060\n",
      "1605 / 3060\n",
      "1606 / 3060\n",
      "1607 / 3060\n",
      "1608 / 3060\n",
      "1609 / 3060\n",
      "1610 / 3060\n",
      "1611 / 3060\n",
      "1612 / 3060\n",
      "1613 / 3060\n",
      "1614 / 3060\n",
      "1615 / 3060\n",
      "1616 / 3060\n",
      "1617 / 3060\n",
      "1618 / 3060\n",
      "1619 / 3060\n",
      "1620 / 3060\n",
      "1621 / 3060\n",
      "1622 / 3060\n",
      "1623 / 3060\n",
      "1624 / 3060\n",
      "1625 / 3060\n",
      "1626 / 3060\n",
      "1627 / 3060\n",
      "1628 / 3060\n",
      "1629 / 3060\n",
      "1630 / 3060\n",
      "1631 / 3060\n",
      "1632 / 3060\n",
      "1633 / 3060\n",
      "1634 / 3060\n",
      "1635 / 3060\n",
      "1636 / 3060\n",
      "1637 / 3060\n",
      "1638 / 3060\n",
      "1639 / 3060\n",
      "1640 / 3060\n",
      "1641 / 3060\n",
      "1642 / 3060\n",
      "1643 / 3060\n",
      "1644 / 3060\n",
      "1645 / 3060\n",
      "1646 / 3060\n",
      "1647 / 3060\n",
      "1648 / 3060\n",
      "1649 / 3060\n",
      "1650 / 3060\n",
      "1651 / 3060\n",
      "1652 / 3060\n",
      "1653 / 3060\n",
      "1654 / 3060\n",
      "1655 / 3060\n",
      "1656 / 3060\n",
      "1657 / 3060\n",
      "1658 / 3060\n",
      "1659 / 3060\n",
      "1660 / 3060\n",
      "1661 / 3060\n",
      "1662 / 3060\n",
      "1663 / 3060\n",
      "1664 / 3060\n",
      "1665 / 3060\n",
      "1666 / 3060\n",
      "1667 / 3060\n",
      "1668 / 3060\n",
      "1669 / 3060\n",
      "1670 / 3060\n",
      "1671 / 3060\n",
      "1672 / 3060\n",
      "1673 / 3060\n",
      "1674 / 3060\n",
      "1675 / 3060\n",
      "1676 / 3060\n",
      "1677 / 3060\n",
      "1678 / 3060\n",
      "1679 / 3060\n",
      "1680 / 3060\n",
      "1681 / 3060\n",
      "1682 / 3060\n",
      "1683 / 3060\n",
      "1684 / 3060\n",
      "1685 / 3060\n",
      "1686 / 3060\n",
      "1687 / 3060\n",
      "1688 / 3060\n",
      "1689 / 3060\n",
      "1690 / 3060\n",
      "1691 / 3060\n",
      "1692 / 3060\n",
      "1693 / 3060\n",
      "1694 / 3060\n",
      "1695 / 3060\n",
      "1696 / 3060\n",
      "1697 / 3060\n",
      "1698 / 3060\n",
      "1699 / 3060\n",
      "1700 / 3060\n",
      "1701 / 3060\n",
      "1702 / 3060\n",
      "1703 / 3060\n",
      "1704 / 3060\n",
      "1705 / 3060\n",
      "1706 / 3060\n",
      "1707 / 3060\n",
      "1708 / 3060\n",
      "1709 / 3060\n",
      "1710 / 3060\n",
      "1711 / 3060\n",
      "1712 / 3060\n",
      "1713 / 3060\n",
      "1714 / 3060\n",
      "1715 / 3060\n",
      "1716 / 3060\n",
      "1717 / 3060\n",
      "1718 / 3060\n",
      "1719 / 3060\n",
      "1720 / 3060\n",
      "1721 / 3060\n",
      "1722 / 3060\n",
      "1723 / 3060\n",
      "1724 / 3060\n",
      "1725 / 3060\n",
      "1726 / 3060\n",
      "1727 / 3060\n",
      "1728 / 3060\n",
      "1729 / 3060\n",
      "1730 / 3060\n",
      "1731 / 3060\n",
      "1732 / 3060\n",
      "1733 / 3060\n",
      "1734 / 3060\n",
      "1735 / 3060\n",
      "1736 / 3060\n",
      "1737 / 3060\n",
      "1738 / 3060\n",
      "1739 / 3060\n",
      "1740 / 3060\n",
      "1741 / 3060\n",
      "1742 / 3060\n",
      "1743 / 3060\n",
      "1744 / 3060\n",
      "1745 / 3060\n",
      "1746 / 3060\n",
      "1747 / 3060\n",
      "1748 / 3060\n",
      "1749 / 3060\n",
      "1750 / 3060\n",
      "1751 / 3060\n",
      "1752 / 3060\n",
      "1753 / 3060\n",
      "1754 / 3060\n",
      "1755 / 3060\n",
      "1756 / 3060\n",
      "1757 / 3060\n",
      "1758 / 3060\n",
      "1759 / 3060\n",
      "1760 / 3060\n",
      "1761 / 3060\n",
      "1762 / 3060\n",
      "1763 / 3060\n",
      "1764 / 3060\n",
      "1765 / 3060\n",
      "1766 / 3060\n",
      "1767 / 3060\n",
      "1768 / 3060\n",
      "1769 / 3060\n",
      "1770 / 3060\n",
      "1771 / 3060\n",
      "1772 / 3060\n",
      "1773 / 3060\n",
      "1774 / 3060\n",
      "1775 / 3060\n",
      "1776 / 3060\n",
      "1777 / 3060\n",
      "1778 / 3060\n",
      "1779 / 3060\n",
      "1780 / 3060\n",
      "1781 / 3060\n",
      "1782 / 3060\n",
      "1783 / 3060\n",
      "1784 / 3060\n",
      "1785 / 3060\n",
      "1786 / 3060\n",
      "1787 / 3060\n",
      "1788 / 3060\n",
      "1789 / 3060\n",
      "1790 / 3060\n",
      "1791 / 3060\n",
      "1792 / 3060\n",
      "1793 / 3060\n",
      "1794 / 3060\n",
      "1795 / 3060\n",
      "1796 / 3060\n",
      "1797 / 3060\n",
      "1798 / 3060\n",
      "1799 / 3060\n",
      "1800 / 3060\n",
      "1801 / 3060\n",
      "1802 / 3060\n",
      "1803 / 3060\n",
      "1804 / 3060\n",
      "1805 / 3060\n",
      "1806 / 3060\n",
      "1807 / 3060\n",
      "1808 / 3060\n",
      "1809 / 3060\n",
      "1810 / 3060\n",
      "1811 / 3060\n",
      "1812 / 3060\n",
      "1813 / 3060\n",
      "1814 / 3060\n",
      "1815 / 3060\n",
      "1816 / 3060\n",
      "1817 / 3060\n",
      "1818 / 3060\n",
      "1819 / 3060\n",
      "1820 / 3060\n",
      "1821 / 3060\n",
      "1822 / 3060\n",
      "1823 / 3060\n",
      "1824 / 3060\n",
      "1825 / 3060\n",
      "1826 / 3060\n",
      "1827 / 3060\n",
      "1828 / 3060\n",
      "1829 / 3060\n",
      "1830 / 3060\n",
      "1831 / 3060\n",
      "1832 / 3060\n",
      "1833 / 3060\n",
      "1834 / 3060\n",
      "1835 / 3060\n",
      "1836 / 3060\n",
      "1837 / 3060\n",
      "1838 / 3060\n",
      "1839 / 3060\n",
      "1840 / 3060\n",
      "1841 / 3060\n",
      "1842 / 3060\n",
      "1843 / 3060\n",
      "1844 / 3060\n",
      "1845 / 3060\n",
      "1846 / 3060\n",
      "1847 / 3060\n",
      "1848 / 3060\n",
      "1849 / 3060\n",
      "1850 / 3060\n",
      "1851 / 3060\n",
      "1852 / 3060\n",
      "1853 / 3060\n",
      "1854 / 3060\n",
      "1855 / 3060\n",
      "1856 / 3060\n",
      "1857 / 3060\n",
      "1858 / 3060\n",
      "1859 / 3060\n",
      "1860 / 3060\n",
      "1861 / 3060\n",
      "1862 / 3060\n",
      "1863 / 3060\n",
      "1864 / 3060\n",
      "1865 / 3060\n",
      "1866 / 3060\n",
      "1867 / 3060\n",
      "1868 / 3060\n",
      "1869 / 3060\n",
      "1870 / 3060\n",
      "1871 / 3060\n",
      "1872 / 3060\n",
      "1873 / 3060\n",
      "1874 / 3060\n",
      "1875 / 3060\n",
      "1876 / 3060\n",
      "1877 / 3060\n",
      "1878 / 3060\n",
      "1879 / 3060\n",
      "1880 / 3060\n",
      "1881 / 3060\n",
      "1882 / 3060\n",
      "1883 / 3060\n",
      "1884 / 3060\n",
      "1885 / 3060\n",
      "1886 / 3060\n",
      "1887 / 3060\n",
      "1888 / 3060\n",
      "1889 / 3060\n",
      "1890 / 3060\n",
      "1891 / 3060\n",
      "1892 / 3060\n",
      "1893 / 3060\n",
      "1894 / 3060\n",
      "1895 / 3060\n",
      "1896 / 3060\n",
      "1897 / 3060\n",
      "1898 / 3060\n",
      "1899 / 3060\n",
      "1900 / 3060\n",
      "1901 / 3060\n",
      "1902 / 3060\n",
      "1903 / 3060\n",
      "1904 / 3060\n",
      "1905 / 3060\n",
      "1906 / 3060\n",
      "1907 / 3060\n",
      "1908 / 3060\n",
      "1909 / 3060\n",
      "1910 / 3060\n",
      "1911 / 3060\n",
      "1912 / 3060\n",
      "1913 / 3060\n",
      "1914 / 3060\n",
      "1915 / 3060\n",
      "1916 / 3060\n",
      "1917 / 3060\n",
      "1918 / 3060\n",
      "1919 / 3060\n",
      "1920 / 3060\n",
      "1921 / 3060\n",
      "1922 / 3060\n",
      "1923 / 3060\n",
      "1924 / 3060\n",
      "1925 / 3060\n",
      "1926 / 3060\n",
      "1927 / 3060\n",
      "1928 / 3060\n",
      "1929 / 3060\n",
      "1930 / 3060\n",
      "1931 / 3060\n",
      "1932 / 3060\n",
      "1933 / 3060\n",
      "1934 / 3060\n",
      "1935 / 3060\n",
      "1936 / 3060\n",
      "1937 / 3060\n",
      "1938 / 3060\n",
      "1939 / 3060\n",
      "1940 / 3060\n",
      "1941 / 3060\n",
      "1942 / 3060\n",
      "1943 / 3060\n",
      "1944 / 3060\n",
      "1945 / 3060\n",
      "1946 / 3060\n",
      "1947 / 3060\n",
      "1948 / 3060\n",
      "1949 / 3060\n",
      "1950 / 3060\n",
      "1951 / 3060\n",
      "1952 / 3060\n",
      "1953 / 3060\n",
      "1954 / 3060\n",
      "1955 / 3060\n",
      "1956 / 3060\n",
      "1957 / 3060\n",
      "1958 / 3060\n",
      "1959 / 3060\n",
      "1960 / 3060\n",
      "1961 / 3060\n",
      "1962 / 3060\n",
      "1963 / 3060\n",
      "1964 / 3060\n",
      "1965 / 3060\n",
      "1966 / 3060\n",
      "1967 / 3060\n",
      "1968 / 3060\n",
      "1969 / 3060\n",
      "1970 / 3060\n",
      "1971 / 3060\n",
      "1972 / 3060\n",
      "1973 / 3060\n",
      "1974 / 3060\n",
      "1975 / 3060\n",
      "1976 / 3060\n",
      "1977 / 3060\n",
      "1978 / 3060\n",
      "1979 / 3060\n",
      "1980 / 3060\n",
      "1981 / 3060\n",
      "1982 / 3060\n",
      "1983 / 3060\n",
      "1984 / 3060\n",
      "1985 / 3060\n",
      "1986 / 3060\n",
      "1987 / 3060\n",
      "1988 / 3060\n",
      "1989 / 3060\n",
      "1990 / 3060\n",
      "1991 / 3060\n",
      "1992 / 3060\n",
      "1993 / 3060\n",
      "1994 / 3060\n",
      "1995 / 3060\n",
      "1996 / 3060\n",
      "1997 / 3060\n",
      "1998 / 3060\n",
      "1999 / 3060\n",
      "2000 / 3060\n",
      "2001 / 3060\n",
      "2002 / 3060\n",
      "2003 / 3060\n",
      "2004 / 3060\n",
      "2005 / 3060\n",
      "2006 / 3060\n",
      "2007 / 3060\n",
      "2008 / 3060\n",
      "2009 / 3060\n",
      "2010 / 3060\n",
      "2011 / 3060\n",
      "2012 / 3060\n",
      "2013 / 3060\n",
      "2014 / 3060\n",
      "2015 / 3060\n",
      "2016 / 3060\n",
      "2017 / 3060\n",
      "2018 / 3060\n",
      "2019 / 3060\n",
      "2020 / 3060\n",
      "2021 / 3060\n",
      "2022 / 3060\n",
      "2023 / 3060\n",
      "2024 / 3060\n",
      "2025 / 3060\n",
      "2026 / 3060\n",
      "2027 / 3060\n",
      "2028 / 3060\n",
      "2029 / 3060\n",
      "2030 / 3060\n",
      "2031 / 3060\n",
      "2032 / 3060\n",
      "2033 / 3060\n",
      "2034 / 3060\n",
      "2035 / 3060\n",
      "2036 / 3060\n",
      "2037 / 3060\n",
      "2038 / 3060\n",
      "2039 / 3060\n",
      "2040 / 3060\n",
      "2041 / 3060\n",
      "2042 / 3060\n",
      "2043 / 3060\n",
      "2044 / 3060\n",
      "2045 / 3060\n",
      "2046 / 3060\n",
      "2047 / 3060\n",
      "2048 / 3060\n",
      "2049 / 3060\n",
      "2050 / 3060\n",
      "2051 / 3060\n",
      "2052 / 3060\n",
      "2053 / 3060\n",
      "2054 / 3060\n",
      "2055 / 3060\n",
      "2056 / 3060\n",
      "2057 / 3060\n",
      "2058 / 3060\n",
      "2059 / 3060\n",
      "2060 / 3060\n",
      "2061 / 3060\n",
      "2062 / 3060\n",
      "2063 / 3060\n",
      "2064 / 3060\n",
      "2065 / 3060\n",
      "2066 / 3060\n",
      "2067 / 3060\n",
      "2068 / 3060\n",
      "2069 / 3060\n",
      "2070 / 3060\n",
      "2071 / 3060\n",
      "2072 / 3060\n",
      "2073 / 3060\n",
      "2074 / 3060\n",
      "2075 / 3060\n",
      "2076 / 3060\n",
      "2077 / 3060\n",
      "2078 / 3060\n",
      "2079 / 3060\n",
      "2080 / 3060\n",
      "2081 / 3060\n",
      "2082 / 3060\n",
      "2083 / 3060\n",
      "2084 / 3060\n",
      "2085 / 3060\n",
      "2086 / 3060\n",
      "2087 / 3060\n",
      "2088 / 3060\n",
      "2089 / 3060\n",
      "2090 / 3060\n",
      "2091 / 3060\n",
      "2092 / 3060\n",
      "2093 / 3060\n",
      "2094 / 3060\n",
      "2095 / 3060\n",
      "2096 / 3060\n",
      "2097 / 3060\n",
      "2098 / 3060\n",
      "2099 / 3060\n",
      "2100 / 3060\n",
      "2101 / 3060\n",
      "2102 / 3060\n",
      "2103 / 3060\n",
      "2104 / 3060\n",
      "2105 / 3060\n",
      "2106 / 3060\n",
      "2107 / 3060\n",
      "2108 / 3060\n",
      "2109 / 3060\n",
      "2110 / 3060\n",
      "2111 / 3060\n",
      "2112 / 3060\n",
      "2113 / 3060\n",
      "2114 / 3060\n",
      "2115 / 3060\n",
      "2116 / 3060\n",
      "2117 / 3060\n",
      "2118 / 3060\n",
      "2119 / 3060\n",
      "2120 / 3060\n",
      "2121 / 3060\n",
      "2122 / 3060\n",
      "2123 / 3060\n",
      "2124 / 3060\n",
      "2125 / 3060\n",
      "2126 / 3060\n",
      "2127 / 3060\n",
      "2128 / 3060\n",
      "2129 / 3060\n",
      "2130 / 3060\n",
      "2131 / 3060\n",
      "2132 / 3060\n",
      "2133 / 3060\n",
      "2134 / 3060\n",
      "2135 / 3060\n",
      "2136 / 3060\n",
      "2137 / 3060\n",
      "2138 / 3060\n",
      "2139 / 3060\n",
      "2140 / 3060\n",
      "2141 / 3060\n",
      "2142 / 3060\n",
      "2143 / 3060\n",
      "2144 / 3060\n",
      "2145 / 3060\n",
      "2146 / 3060\n",
      "2147 / 3060\n",
      "2148 / 3060\n",
      "2149 / 3060\n",
      "2150 / 3060\n",
      "2151 / 3060\n",
      "2152 / 3060\n",
      "2153 / 3060\n",
      "2154 / 3060\n",
      "2155 / 3060\n",
      "2156 / 3060\n",
      "2157 / 3060\n",
      "2158 / 3060\n",
      "2159 / 3060\n",
      "2160 / 3060\n",
      "2161 / 3060\n",
      "2162 / 3060\n",
      "2163 / 3060\n",
      "2164 / 3060\n",
      "2165 / 3060\n",
      "2166 / 3060\n",
      "2167 / 3060\n",
      "2168 / 3060\n",
      "2169 / 3060\n",
      "2170 / 3060\n",
      "2171 / 3060\n",
      "2172 / 3060\n",
      "2173 / 3060\n",
      "2174 / 3060\n",
      "2175 / 3060\n",
      "2176 / 3060\n",
      "2177 / 3060\n",
      "2178 / 3060\n",
      "2179 / 3060\n",
      "2180 / 3060\n",
      "2181 / 3060\n",
      "2182 / 3060\n",
      "2183 / 3060\n",
      "2184 / 3060\n",
      "2185 / 3060\n",
      "2186 / 3060\n",
      "2187 / 3060\n",
      "2188 / 3060\n",
      "2189 / 3060\n",
      "2190 / 3060\n",
      "2191 / 3060\n",
      "2192 / 3060\n",
      "2193 / 3060\n",
      "2194 / 3060\n",
      "2195 / 3060\n",
      "2196 / 3060\n",
      "2197 / 3060\n",
      "2198 / 3060\n",
      "2199 / 3060\n",
      "2200 / 3060\n",
      "2201 / 3060\n",
      "2202 / 3060\n",
      "2203 / 3060\n",
      "2204 / 3060\n",
      "2205 / 3060\n",
      "2206 / 3060\n",
      "2207 / 3060\n",
      "2208 / 3060\n",
      "2209 / 3060\n",
      "2210 / 3060\n",
      "2211 / 3060\n",
      "2212 / 3060\n",
      "2213 / 3060\n",
      "2214 / 3060\n",
      "2215 / 3060\n",
      "2216 / 3060\n",
      "2217 / 3060\n",
      "2218 / 3060\n",
      "2219 / 3060\n",
      "2220 / 3060\n",
      "2221 / 3060\n",
      "2222 / 3060\n",
      "2223 / 3060\n",
      "2224 / 3060\n",
      "2225 / 3060\n",
      "2226 / 3060\n",
      "2227 / 3060\n",
      "2228 / 3060\n",
      "2229 / 3060\n",
      "2230 / 3060\n",
      "2231 / 3060\n",
      "2232 / 3060\n",
      "2233 / 3060\n",
      "2234 / 3060\n",
      "2235 / 3060\n",
      "2236 / 3060\n",
      "2237 / 3060\n",
      "2238 / 3060\n",
      "2239 / 3060\n",
      "2240 / 3060\n",
      "2241 / 3060\n",
      "2242 / 3060\n",
      "2243 / 3060\n",
      "2244 / 3060\n",
      "2245 / 3060\n",
      "2246 / 3060\n",
      "2247 / 3060\n",
      "2248 / 3060\n",
      "2249 / 3060\n",
      "2250 / 3060\n",
      "2251 / 3060\n",
      "2252 / 3060\n",
      "2253 / 3060\n",
      "2254 / 3060\n",
      "2255 / 3060\n",
      "2256 / 3060\n",
      "2257 / 3060\n",
      "2258 / 3060\n",
      "2259 / 3060\n",
      "2260 / 3060\n",
      "2261 / 3060\n",
      "2262 / 3060\n",
      "2263 / 3060\n",
      "2264 / 3060\n",
      "2265 / 3060\n",
      "2266 / 3060\n",
      "2267 / 3060\n",
      "2268 / 3060\n",
      "2269 / 3060\n",
      "2270 / 3060\n",
      "2271 / 3060\n",
      "2272 / 3060\n",
      "2273 / 3060\n",
      "2274 / 3060\n",
      "2275 / 3060\n",
      "2276 / 3060\n",
      "2277 / 3060\n",
      "2278 / 3060\n",
      "2279 / 3060\n",
      "2280 / 3060\n",
      "2281 / 3060\n",
      "2282 / 3060\n",
      "2283 / 3060\n",
      "2284 / 3060\n",
      "2285 / 3060\n",
      "2286 / 3060\n",
      "2287 / 3060\n",
      "2288 / 3060\n",
      "2289 / 3060\n",
      "2290 / 3060\n",
      "2291 / 3060\n",
      "2292 / 3060\n",
      "2293 / 3060\n",
      "2294 / 3060\n",
      "2295 / 3060\n",
      "2296 / 3060\n",
      "2297 / 3060\n",
      "2298 / 3060\n",
      "2299 / 3060\n",
      "2300 / 3060\n",
      "2301 / 3060\n",
      "2302 / 3060\n",
      "2303 / 3060\n",
      "2304 / 3060\n",
      "2305 / 3060\n",
      "2306 / 3060\n",
      "2307 / 3060\n",
      "2308 / 3060\n",
      "2309 / 3060\n",
      "2310 / 3060\n",
      "2311 / 3060\n",
      "2312 / 3060\n",
      "2313 / 3060\n",
      "2314 / 3060\n",
      "2315 / 3060\n",
      "2316 / 3060\n",
      "2317 / 3060\n",
      "2318 / 3060\n",
      "2319 / 3060\n",
      "2320 / 3060\n",
      "2321 / 3060\n",
      "2322 / 3060\n",
      "2323 / 3060\n",
      "2324 / 3060\n",
      "2325 / 3060\n",
      "2326 / 3060\n",
      "2327 / 3060\n",
      "2328 / 3060\n",
      "2329 / 3060\n",
      "2330 / 3060\n",
      "2331 / 3060\n",
      "2332 / 3060\n",
      "2333 / 3060\n",
      "2334 / 3060\n",
      "2335 / 3060\n",
      "2336 / 3060\n",
      "2337 / 3060\n",
      "2338 / 3060\n",
      "2339 / 3060\n",
      "2340 / 3060\n",
      "2341 / 3060\n",
      "2342 / 3060\n",
      "2343 / 3060\n",
      "2344 / 3060\n",
      "2345 / 3060\n",
      "2346 / 3060\n",
      "2347 / 3060\n",
      "2348 / 3060\n",
      "2349 / 3060\n",
      "2350 / 3060\n",
      "2351 / 3060\n",
      "2352 / 3060\n",
      "2353 / 3060\n",
      "2354 / 3060\n",
      "2355 / 3060\n",
      "2356 / 3060\n",
      "2357 / 3060\n",
      "2358 / 3060\n",
      "2359 / 3060\n",
      "2360 / 3060\n",
      "2361 / 3060\n",
      "2362 / 3060\n",
      "2363 / 3060\n",
      "2364 / 3060\n",
      "2365 / 3060\n",
      "2366 / 3060\n",
      "2367 / 3060\n",
      "2368 / 3060\n",
      "2369 / 3060\n",
      "2370 / 3060\n",
      "2371 / 3060\n",
      "2372 / 3060\n",
      "2373 / 3060\n",
      "2374 / 3060\n",
      "2375 / 3060\n",
      "2376 / 3060\n",
      "2377 / 3060\n",
      "2378 / 3060\n",
      "2379 / 3060\n",
      "2380 / 3060\n",
      "2381 / 3060\n",
      "2382 / 3060\n",
      "2383 / 3060\n",
      "2384 / 3060\n",
      "2385 / 3060\n",
      "2386 / 3060\n",
      "2387 / 3060\n",
      "2388 / 3060\n",
      "2389 / 3060\n",
      "2390 / 3060\n",
      "2391 / 3060\n",
      "2392 / 3060\n",
      "2393 / 3060\n",
      "2394 / 3060\n",
      "2395 / 3060\n",
      "2396 / 3060\n",
      "2397 / 3060\n",
      "2398 / 3060\n",
      "2399 / 3060\n",
      "2400 / 3060\n",
      "2401 / 3060\n",
      "2402 / 3060\n",
      "2403 / 3060\n",
      "2404 / 3060\n",
      "2405 / 3060\n",
      "2406 / 3060\n",
      "2407 / 3060\n",
      "2408 / 3060\n",
      "2409 / 3060\n",
      "2410 / 3060\n",
      "2411 / 3060\n",
      "2412 / 3060\n",
      "2413 / 3060\n",
      "2414 / 3060\n",
      "2415 / 3060\n",
      "2416 / 3060\n",
      "2417 / 3060\n",
      "2418 / 3060\n",
      "2419 / 3060\n",
      "2420 / 3060\n",
      "2421 / 3060\n",
      "2422 / 3060\n",
      "2423 / 3060\n",
      "2424 / 3060\n",
      "2425 / 3060\n",
      "2426 / 3060\n",
      "2427 / 3060\n",
      "2428 / 3060\n",
      "2429 / 3060\n",
      "2430 / 3060\n",
      "2431 / 3060\n",
      "2432 / 3060\n",
      "2433 / 3060\n",
      "2434 / 3060\n",
      "2435 / 3060\n",
      "2436 / 3060\n",
      "2437 / 3060\n",
      "2438 / 3060\n",
      "2439 / 3060\n",
      "2440 / 3060\n",
      "2441 / 3060\n",
      "2442 / 3060\n",
      "2443 / 3060\n",
      "2444 / 3060\n",
      "2445 / 3060\n",
      "2446 / 3060\n",
      "2447 / 3060\n",
      "2448 / 3060\n",
      "2449 / 3060\n",
      "2450 / 3060\n",
      "2451 / 3060\n",
      "2452 / 3060\n",
      "2453 / 3060\n",
      "2454 / 3060\n",
      "2455 / 3060\n",
      "2456 / 3060\n",
      "2457 / 3060\n",
      "2458 / 3060\n",
      "2459 / 3060\n",
      "2460 / 3060\n",
      "2461 / 3060\n",
      "2462 / 3060\n",
      "2463 / 3060\n",
      "2464 / 3060\n",
      "2465 / 3060\n",
      "2466 / 3060\n",
      "2467 / 3060\n",
      "2468 / 3060\n",
      "2469 / 3060\n",
      "2470 / 3060\n",
      "2471 / 3060\n",
      "2472 / 3060\n",
      "2473 / 3060\n",
      "2474 / 3060\n",
      "2475 / 3060\n",
      "2476 / 3060\n",
      "2477 / 3060\n",
      "2478 / 3060\n",
      "2479 / 3060\n",
      "2480 / 3060\n",
      "2481 / 3060\n",
      "2482 / 3060\n",
      "2483 / 3060\n",
      "2484 / 3060\n",
      "2485 / 3060\n",
      "2486 / 3060\n",
      "2487 / 3060\n",
      "2488 / 3060\n",
      "2489 / 3060\n",
      "2490 / 3060\n",
      "2491 / 3060\n",
      "2492 / 3060\n",
      "2493 / 3060\n",
      "2494 / 3060\n",
      "2495 / 3060\n",
      "2496 / 3060\n",
      "2497 / 3060\n",
      "2498 / 3060\n",
      "2499 / 3060\n",
      "2500 / 3060\n",
      "2501 / 3060\n",
      "2502 / 3060\n",
      "2503 / 3060\n",
      "2504 / 3060\n",
      "2505 / 3060\n",
      "2506 / 3060\n",
      "2507 / 3060\n",
      "2508 / 3060\n",
      "2509 / 3060\n",
      "2510 / 3060\n",
      "2511 / 3060\n",
      "2512 / 3060\n",
      "2513 / 3060\n",
      "2514 / 3060\n",
      "2515 / 3060\n",
      "2516 / 3060\n",
      "2517 / 3060\n",
      "2518 / 3060\n",
      "2519 / 3060\n",
      "2520 / 3060\n",
      "2521 / 3060\n",
      "2522 / 3060\n",
      "2523 / 3060\n",
      "2524 / 3060\n",
      "2525 / 3060\n",
      "2526 / 3060\n",
      "2527 / 3060\n",
      "2528 / 3060\n",
      "2529 / 3060\n",
      "2530 / 3060\n",
      "2531 / 3060\n",
      "2532 / 3060\n",
      "2533 / 3060\n",
      "2534 / 3060\n",
      "2535 / 3060\n",
      "2536 / 3060\n",
      "2537 / 3060\n",
      "2538 / 3060\n",
      "2539 / 3060\n",
      "2540 / 3060\n",
      "2541 / 3060\n",
      "2542 / 3060\n",
      "2543 / 3060\n",
      "2544 / 3060\n",
      "2545 / 3060\n",
      "2546 / 3060\n",
      "2547 / 3060\n",
      "2548 / 3060\n",
      "2549 / 3060\n",
      "2550 / 3060\n",
      "2551 / 3060\n",
      "2552 / 3060\n",
      "2553 / 3060\n",
      "2554 / 3060\n",
      "2555 / 3060\n",
      "2556 / 3060\n",
      "2557 / 3060\n",
      "2558 / 3060\n",
      "2559 / 3060\n",
      "2560 / 3060\n",
      "2561 / 3060\n",
      "2562 / 3060\n",
      "2563 / 3060\n",
      "2564 / 3060\n",
      "2565 / 3060\n",
      "2566 / 3060\n",
      "2567 / 3060\n",
      "2568 / 3060\n",
      "2569 / 3060\n",
      "2570 / 3060\n",
      "2571 / 3060\n",
      "2572 / 3060\n",
      "2573 / 3060\n",
      "2574 / 3060\n",
      "2575 / 3060\n",
      "2576 / 3060\n",
      "2577 / 3060\n",
      "2578 / 3060\n",
      "2579 / 3060\n",
      "2580 / 3060\n",
      "2581 / 3060\n",
      "2582 / 3060\n",
      "2583 / 3060\n",
      "2584 / 3060\n",
      "2585 / 3060\n",
      "2586 / 3060\n",
      "2587 / 3060\n",
      "2588 / 3060\n",
      "2589 / 3060\n",
      "2590 / 3060\n",
      "2591 / 3060\n",
      "2592 / 3060\n",
      "2593 / 3060\n",
      "2594 / 3060\n",
      "2595 / 3060\n",
      "2596 / 3060\n",
      "2597 / 3060\n",
      "2598 / 3060\n",
      "2599 / 3060\n",
      "2600 / 3060\n",
      "2601 / 3060\n",
      "2602 / 3060\n",
      "2603 / 3060\n",
      "2604 / 3060\n",
      "2605 / 3060\n",
      "2606 / 3060\n",
      "2607 / 3060\n",
      "2608 / 3060\n",
      "2609 / 3060\n",
      "2610 / 3060\n",
      "2611 / 3060\n",
      "2612 / 3060\n",
      "2613 / 3060\n",
      "2614 / 3060\n",
      "2615 / 3060\n",
      "2616 / 3060\n",
      "2617 / 3060\n",
      "2618 / 3060\n",
      "2619 / 3060\n",
      "2620 / 3060\n",
      "2621 / 3060\n",
      "2622 / 3060\n",
      "2623 / 3060\n",
      "2624 / 3060\n",
      "2625 / 3060\n",
      "2626 / 3060\n",
      "2627 / 3060\n",
      "2628 / 3060\n",
      "2629 / 3060\n",
      "2630 / 3060\n",
      "2631 / 3060\n",
      "2632 / 3060\n",
      "2633 / 3060\n",
      "2634 / 3060\n",
      "2635 / 3060\n",
      "2636 / 3060\n",
      "2637 / 3060\n",
      "2638 / 3060\n",
      "2639 / 3060\n",
      "2640 / 3060\n",
      "2641 / 3060\n",
      "2642 / 3060\n",
      "2643 / 3060\n",
      "2644 / 3060\n",
      "2645 / 3060\n",
      "2646 / 3060\n",
      "2647 / 3060\n",
      "2648 / 3060\n",
      "2649 / 3060\n",
      "2650 / 3060\n",
      "2651 / 3060\n",
      "2652 / 3060\n",
      "2653 / 3060\n",
      "2654 / 3060\n",
      "2655 / 3060\n",
      "2656 / 3060\n",
      "2657 / 3060\n",
      "2658 / 3060\n",
      "2659 / 3060\n",
      "2660 / 3060\n",
      "2661 / 3060\n",
      "2662 / 3060\n",
      "2663 / 3060\n",
      "2664 / 3060\n",
      "2665 / 3060\n",
      "2666 / 3060\n",
      "2667 / 3060\n",
      "2668 / 3060\n",
      "2669 / 3060\n",
      "2670 / 3060\n",
      "2671 / 3060\n",
      "2672 / 3060\n",
      "2673 / 3060\n",
      "2674 / 3060\n",
      "2675 / 3060\n",
      "2676 / 3060\n",
      "2677 / 3060\n",
      "2678 / 3060\n",
      "2679 / 3060\n",
      "2680 / 3060\n",
      "2681 / 3060\n",
      "2682 / 3060\n",
      "2683 / 3060\n",
      "2684 / 3060\n",
      "2685 / 3060\n",
      "2686 / 3060\n",
      "2687 / 3060\n",
      "2688 / 3060\n",
      "2689 / 3060\n",
      "2690 / 3060\n",
      "2691 / 3060\n",
      "2692 / 3060\n",
      "2693 / 3060\n",
      "2694 / 3060\n",
      "2695 / 3060\n",
      "2696 / 3060\n",
      "2697 / 3060\n",
      "2698 / 3060\n",
      "2699 / 3060\n",
      "2700 / 3060\n",
      "2701 / 3060\n",
      "2702 / 3060\n",
      "2703 / 3060\n",
      "2704 / 3060\n",
      "2705 / 3060\n",
      "2706 / 3060\n",
      "2707 / 3060\n",
      "2708 / 3060\n",
      "2709 / 3060\n",
      "2710 / 3060\n",
      "2711 / 3060\n",
      "2712 / 3060\n",
      "2713 / 3060\n",
      "2714 / 3060\n",
      "2715 / 3060\n",
      "2716 / 3060\n",
      "2717 / 3060\n",
      "2718 / 3060\n",
      "2719 / 3060\n",
      "2720 / 3060\n",
      "2721 / 3060\n",
      "2722 / 3060\n",
      "2723 / 3060\n",
      "2724 / 3060\n",
      "2725 / 3060\n",
      "2726 / 3060\n",
      "2727 / 3060\n",
      "2728 / 3060\n",
      "2729 / 3060\n",
      "2730 / 3060\n",
      "2731 / 3060\n",
      "2732 / 3060\n",
      "2733 / 3060\n",
      "2734 / 3060\n",
      "2735 / 3060\n",
      "2736 / 3060\n",
      "2737 / 3060\n",
      "2738 / 3060\n",
      "2739 / 3060\n",
      "2740 / 3060\n",
      "2741 / 3060\n",
      "2742 / 3060\n",
      "2743 / 3060\n",
      "2744 / 3060\n",
      "2745 / 3060\n",
      "2746 / 3060\n",
      "2747 / 3060\n",
      "2748 / 3060\n",
      "2749 / 3060\n",
      "2750 / 3060\n",
      "2751 / 3060\n",
      "2752 / 3060\n",
      "2753 / 3060\n",
      "2754 / 3060\n",
      "2755 / 3060\n",
      "2756 / 3060\n",
      "2757 / 3060\n",
      "2758 / 3060\n",
      "2759 / 3060\n",
      "2760 / 3060\n",
      "2761 / 3060\n",
      "2762 / 3060\n",
      "2763 / 3060\n",
      "2764 / 3060\n",
      "2765 / 3060\n",
      "2766 / 3060\n",
      "2767 / 3060\n",
      "2768 / 3060\n",
      "2769 / 3060\n",
      "2770 / 3060\n",
      "2771 / 3060\n",
      "2772 / 3060\n",
      "2773 / 3060\n",
      "2774 / 3060\n",
      "2775 / 3060\n",
      "2776 / 3060\n",
      "2777 / 3060\n",
      "2778 / 3060\n",
      "2779 / 3060\n",
      "2780 / 3060\n",
      "2781 / 3060\n",
      "2782 / 3060\n",
      "2783 / 3060\n",
      "2784 / 3060\n",
      "2785 / 3060\n",
      "2786 / 3060\n",
      "2787 / 3060\n",
      "2788 / 3060\n",
      "2789 / 3060\n",
      "2790 / 3060\n",
      "2791 / 3060\n",
      "2792 / 3060\n",
      "2793 / 3060\n",
      "2794 / 3060\n",
      "2795 / 3060\n",
      "2796 / 3060\n",
      "2797 / 3060\n",
      "2798 / 3060\n",
      "2799 / 3060\n",
      "2800 / 3060\n",
      "2801 / 3060\n",
      "2802 / 3060\n",
      "2803 / 3060\n",
      "2804 / 3060\n",
      "2805 / 3060\n",
      "2806 / 3060\n",
      "2807 / 3060\n",
      "2808 / 3060\n",
      "2809 / 3060\n",
      "2810 / 3060\n",
      "2811 / 3060\n",
      "2812 / 3060\n",
      "2813 / 3060\n",
      "2814 / 3060\n",
      "2815 / 3060\n",
      "2816 / 3060\n",
      "2817 / 3060\n",
      "2818 / 3060\n",
      "2819 / 3060\n",
      "2820 / 3060\n",
      "2821 / 3060\n",
      "2822 / 3060\n",
      "2823 / 3060\n",
      "2824 / 3060\n",
      "2825 / 3060\n",
      "2826 / 3060\n",
      "2827 / 3060\n",
      "2828 / 3060\n",
      "2829 / 3060\n",
      "2830 / 3060\n",
      "2831 / 3060\n",
      "2832 / 3060\n",
      "2833 / 3060\n",
      "2834 / 3060\n",
      "2835 / 3060\n",
      "2836 / 3060\n",
      "2837 / 3060\n",
      "2838 / 3060\n",
      "2839 / 3060\n",
      "2840 / 3060\n",
      "2841 / 3060\n",
      "2842 / 3060\n",
      "2843 / 3060\n",
      "2844 / 3060\n",
      "2845 / 3060\n",
      "2846 / 3060\n",
      "2847 / 3060\n",
      "2848 / 3060\n",
      "2849 / 3060\n",
      "2850 / 3060\n",
      "2851 / 3060\n",
      "2852 / 3060\n",
      "2853 / 3060\n",
      "2854 / 3060\n",
      "2855 / 3060\n",
      "2856 / 3060\n",
      "2857 / 3060\n",
      "2858 / 3060\n",
      "2859 / 3060\n",
      "2860 / 3060\n",
      "2861 / 3060\n",
      "2862 / 3060\n",
      "2863 / 3060\n",
      "2864 / 3060\n",
      "2865 / 3060\n",
      "2866 / 3060\n",
      "2867 / 3060\n",
      "2868 / 3060\n",
      "2869 / 3060\n",
      "2870 / 3060\n",
      "2871 / 3060\n",
      "2872 / 3060\n",
      "2873 / 3060\n",
      "2874 / 3060\n",
      "2875 / 3060\n",
      "2876 / 3060\n",
      "2877 / 3060\n",
      "2878 / 3060\n",
      "2879 / 3060\n",
      "2880 / 3060\n",
      "2881 / 3060\n",
      "2882 / 3060\n",
      "2883 / 3060\n",
      "2884 / 3060\n",
      "2885 / 3060\n",
      "2886 / 3060\n",
      "2887 / 3060\n",
      "2888 / 3060\n",
      "2889 / 3060\n",
      "2890 / 3060\n",
      "2891 / 3060\n",
      "2892 / 3060\n",
      "2893 / 3060\n",
      "2894 / 3060\n",
      "2895 / 3060\n",
      "2896 / 3060\n",
      "2897 / 3060\n",
      "2898 / 3060\n",
      "2899 / 3060\n",
      "2900 / 3060\n",
      "2901 / 3060\n",
      "2902 / 3060\n",
      "2903 / 3060\n",
      "2904 / 3060\n",
      "2905 / 3060\n",
      "2906 / 3060\n",
      "2907 / 3060\n",
      "2908 / 3060\n",
      "2909 / 3060\n",
      "2910 / 3060\n",
      "2911 / 3060\n",
      "2912 / 3060\n",
      "2913 / 3060\n",
      "2914 / 3060\n",
      "2915 / 3060\n",
      "2916 / 3060\n",
      "2917 / 3060\n",
      "2918 / 3060\n",
      "2919 / 3060\n",
      "2920 / 3060\n",
      "2921 / 3060\n",
      "2922 / 3060\n",
      "2923 / 3060\n",
      "2924 / 3060\n",
      "2925 / 3060\n",
      "2926 / 3060\n",
      "2927 / 3060\n",
      "2928 / 3060\n",
      "2929 / 3060\n",
      "2930 / 3060\n",
      "2931 / 3060\n",
      "2932 / 3060\n",
      "2933 / 3060\n",
      "2934 / 3060\n",
      "2935 / 3060\n",
      "2936 / 3060\n",
      "2937 / 3060\n",
      "2938 / 3060\n",
      "2939 / 3060\n",
      "2940 / 3060\n",
      "2941 / 3060\n",
      "2942 / 3060\n",
      "2943 / 3060\n",
      "2944 / 3060\n",
      "2945 / 3060\n",
      "2946 / 3060\n",
      "2947 / 3060\n",
      "2948 / 3060\n",
      "2949 / 3060\n",
      "2950 / 3060\n",
      "2951 / 3060\n",
      "2952 / 3060\n",
      "2953 / 3060\n",
      "2954 / 3060\n",
      "2955 / 3060\n",
      "2956 / 3060\n",
      "2957 / 3060\n",
      "2958 / 3060\n",
      "2959 / 3060\n",
      "2960 / 3060\n",
      "2961 / 3060\n",
      "2962 / 3060\n",
      "2963 / 3060\n",
      "2964 / 3060\n",
      "2965 / 3060\n",
      "2966 / 3060\n",
      "2967 / 3060\n",
      "2968 / 3060\n",
      "2969 / 3060\n",
      "2970 / 3060\n",
      "2971 / 3060\n",
      "2972 / 3060\n",
      "2973 / 3060\n",
      "2974 / 3060\n",
      "2975 / 3060\n",
      "2976 / 3060\n",
      "2977 / 3060\n",
      "2978 / 3060\n",
      "2979 / 3060\n",
      "2980 / 3060\n",
      "2981 / 3060\n",
      "2982 / 3060\n",
      "2983 / 3060\n",
      "2984 / 3060\n",
      "2985 / 3060\n",
      "2986 / 3060\n",
      "2987 / 3060\n",
      "2988 / 3060\n",
      "2989 / 3060\n",
      "2990 / 3060\n",
      "2991 / 3060\n",
      "2992 / 3060\n",
      "2993 / 3060\n",
      "2994 / 3060\n",
      "2995 / 3060\n",
      "2996 / 3060\n",
      "2997 / 3060\n",
      "2998 / 3060\n",
      "2999 / 3060\n",
      "3000 / 3060\n",
      "3001 / 3060\n",
      "3002 / 3060\n",
      "3003 / 3060\n",
      "3004 / 3060\n",
      "3005 / 3060\n",
      "3006 / 3060\n",
      "3007 / 3060\n",
      "3008 / 3060\n",
      "3009 / 3060\n",
      "3010 / 3060\n",
      "3011 / 3060\n",
      "3012 / 3060\n",
      "3013 / 3060\n",
      "3014 / 3060\n",
      "3015 / 3060\n",
      "3016 / 3060\n",
      "3017 / 3060\n",
      "3018 / 3060\n",
      "3019 / 3060\n",
      "3020 / 3060\n",
      "3021 / 3060\n",
      "3022 / 3060\n",
      "3023 / 3060\n",
      "3024 / 3060\n",
      "3025 / 3060\n",
      "3026 / 3060\n",
      "3027 / 3060\n",
      "3028 / 3060\n",
      "3029 / 3060\n",
      "3030 / 3060\n",
      "3031 / 3060\n",
      "3032 / 3060\n",
      "3033 / 3060\n",
      "3034 / 3060\n",
      "3035 / 3060\n",
      "3036 / 3060\n",
      "3037 / 3060\n",
      "3038 / 3060\n",
      "3039 / 3060\n",
      "3040 / 3060\n",
      "3041 / 3060\n",
      "3042 / 3060\n",
      "3043 / 3060\n",
      "3044 / 3060\n",
      "3045 / 3060\n",
      "3046 / 3060\n",
      "3047 / 3060\n",
      "3048 / 3060\n",
      "3049 / 3060\n",
      "3050 / 3060\n",
      "3051 / 3060\n",
      "3052 / 3060\n",
      "3053 / 3060\n",
      "3054 / 3060\n",
      "3055 / 3060\n",
      "3056 / 3060\n",
      "3057 / 3060\n",
      "3058 / 3060\n",
      "3059 / 3060\n"
     ]
    }
   ],
   "source": [
    "# unload all of the data from pytorch dataset to numpy and save\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "curr = 0\n",
    "total = len(tess_data)\n",
    "print(\"Loading validation data:\")\n",
    "for x_data, y_data in tess_data:\n",
    "    print(curr, \"/\", total)\n",
    "    if not isinstance(y_data, np.float64):\n",
    "        print(f'ERROR: {y_data}')\n",
    "    curr += 1\n",
    "    \n",
    "    x.append(x_data)\n",
    "    y.append(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ff461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_dataset(data, target, train_ratio, validation_ratio, test_ratio):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=1-train_ratio, shuffle=True, stratify=target)\n",
    "    \n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), shuffle=True, stratify=y_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1536989",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_val, y_val = train_test_split_dataset(x, y, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed95f70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    254\n",
       "0.0     52\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(y_val).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beabb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.asarray(x_val, dtype=object)\n",
    "y_val = np.asarray(y_val, dtype=np.float32)\n",
    "x_train = np.asarray(x_train, dtype=object)\n",
    "y_train = np.asarray(y_train, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920a1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val shape: (306, 5)\n",
      "x_val[0][0] shape: (61,)\n",
      "x_val[0][1] shape: (201,)\n",
      "x_val[0][2] shape: (61,)\n",
      "x_val[0][3] shape: (201,)\n",
      "x_val[0][4] shape: (17,)\n",
      "y_val shape: (306,)\n"
     ]
    }
   ],
   "source": [
    "print(f'x_val shape: {x_val.shape}')\n",
    "print(f'x_val[0][0] shape: {x_val[0][0].shape}')\n",
    "print(f'x_val[0][1] shape: {x_val[0][1].shape}')\n",
    "print(f'x_val[0][2] shape: {x_val[0][2].shape}')\n",
    "print(f'x_val[0][3] shape: {x_val[0][3].shape}')\n",
    "print(f'x_val[0][4] shape: {x_val[0][4].shape}')\n",
    "print(f'y_val shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1df17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "# save the validation data that was loaded\n",
    "save(\"./outputs/val_x_data.npy\", x_val)\n",
    "save(\"./outputs/val_y_data.npy\", y_val)\n",
    "save(\"./outputs/train_x_data.npy\", x_train)\n",
    "save(\"./outputs/train_y_data.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3109f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "x2_val = load(\"./outputs/val_x_data.npy\", allow_pickle=True)\n",
    "y2_val = load(\"./outputs/val_y_data.npy\", allow_pickle=True)\n",
    "x2_train = load(\"./outputs/train_x_data.npy\", allow_pickle=True)\n",
    "y2_train = load(\"./outputs/train_y_data.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c4304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 5)\n",
      "(306,)\n"
     ]
    }
   ],
   "source": [
    "print(x2_val.shape)\n",
    "print(y2_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "823adfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, Nadam, SGD\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d43b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes to the Extranet model:\n",
      "X_LOCAL_SHAPE: (61,)\n",
      "X_GLOBAL_SHAPE: (201,)\n",
      "X_LOCAL_CEN_SHAPE: (61,)\n",
      "X_GLOBAL_CEN_SHAPE: (201,)\n",
      "X_STARPARS_SHAPE: (17,)\n",
      "Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shapes to the Extranet model:\")\n",
    "X_LOCAL_SHAPE = data[0][0].shape\n",
    "print(\"X_LOCAL_SHAPE:\", X_LOCAL_SHAPE)\n",
    "X_GLOBAL_SHAPE = data[0][1].shape\n",
    "print(\"X_GLOBAL_SHAPE:\", X_GLOBAL_SHAPE)\n",
    "X_LOCAL_CEN_SHAPE = data[0][2].shape\n",
    "print(\"X_LOCAL_CEN_SHAPE:\", X_LOCAL_CEN_SHAPE)\n",
    "X_GLOBAL_CEN_SHAPE = data[0][3].shape\n",
    "print(\"X_GLOBAL_CEN_SHAPE:\", X_GLOBAL_CEN_SHAPE)\n",
    "X_STARPARS_SHAPE = data[0][4].shape\n",
    "print(\"X_STARPARS_SHAPE:\", X_STARPARS_SHAPE)\n",
    "print(\"Label:\", data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65eacbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam optimizer learning rate: 1e-05\n"
     ]
    }
   ],
   "source": [
    "FC_LOCAL_OUT_SHAPE = None\n",
    "FC_GLOBAL_OUT_SHAPE = None\n",
    "R_LEARN = 1e-5\n",
    "print(\"Adam optimizer learning rate:\", R_LEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4598ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected global network used for Extranet\n",
    "def create_fc_global():\n",
    "    in_layer = Input(shape=(X_GLOBAL_SHAPE[0], 2))\n",
    "    # unclear what the input shape should be\n",
    "    # extranet has an input of 2 and concatenate the 2 inputs along dim 1\n",
    "\n",
    "    fc = Conv1D(16, kernel_size=5, strides=1, padding='same')(in_layer)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = Conv1D(16, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = MaxPool1D(pool_size=5, strides=2)(fc)\n",
    "\n",
    "    fc = Conv1D(32, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = Conv1D(32, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = MaxPool1D(pool_size=5, strides=2)(fc)\n",
    "\n",
    "    fc = Conv1D(64, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = Conv1D(64, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = MaxPool1D(pool_size=5, strides=2)(fc)\n",
    "\n",
    "    fc = Conv1D(128, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = Conv1D(128, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = MaxPool1D(pool_size=5, strides=2)(fc)\n",
    "\n",
    "    fc = Conv1D(256, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = Conv1D(256, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    out_layer = MaxPool1D(pool_size=5, strides=2)(fc)\n",
    "    \n",
    "    # save the shape\n",
    "    FC_GLOBAL_OUT_SHAPE = out_layer.shape\n",
    "\n",
    "    model = Model(inputs=in_layer, outputs=out_layer, name='fully_connected_global')\n",
    "    return model, FC_GLOBAL_OUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba61414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fully_connected_global\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 201, 2)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 201, 16)           176       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 201, 16)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 201, 16)           1296      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 201, 16)           0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 99, 16)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 99, 32)            2592      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 99, 32)            0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 99, 32)            5152      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 99, 32)            0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 48, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 48, 64)            10304     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 48, 64)            0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 48, 64)            20544     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 48, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 22, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 22, 128)           41088     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 22, 128)           0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 22, 128)           82048     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 22, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 9, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 9, 256)            164096    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 9, 256)            0         \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 9, 256)            327936    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 9, 256)            0         \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 3, 256)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 655,232\n",
      "Trainable params: 655,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Output shape: (None, 3, 256)\n"
     ]
    }
   ],
   "source": [
    "fc_global_model, FC_GLOBAL_OUT_SHAPE = create_fc_global()\n",
    "fc_global_model.summary()\n",
    "print()\n",
    "print(\"Output shape:\", FC_GLOBAL_OUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f3f174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(fc_global_model, to_file='fc_global_model.jpg', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd65cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected global network used for Extranet\n",
    "def create_fc_local():\n",
    "    in_layer = Input(shape=(X_LOCAL_SHAPE[0], 2))\n",
    "    # unclear what the input shape should be\n",
    "    # extranet has an input of 2 and concatenate the 2 inputs along dim 1\n",
    "\n",
    "    fc = Conv1D(16, kernel_size=5, strides=1, padding='same')(in_layer)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = Conv1D(16, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = MaxPool1D(pool_size=7, strides=2)(fc)\n",
    "    #fc = Dropout(.25)(fc)\n",
    "\n",
    "    fc = Conv1D(32, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = Conv1D(32, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    out_layer = MaxPool1D(pool_size=7, strides=2)(fc)\n",
    "\n",
    "    # save the shape\n",
    "    FC_LOCAL_OUT_SHAPE = out_layer.shape\n",
    "\n",
    "    model = Model(inputs=in_layer, outputs=out_layer, name='fully_connected_local')\n",
    "    return model, FC_LOCAL_OUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8c34f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fully_connected_local\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 61, 2)]           0         \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 61, 16)            176       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 61, 16)            0         \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 61, 16)            1296      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 61, 16)            0         \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 28, 16)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 28, 32)            2592      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 28, 32)            0         \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 28, 32)            5152      \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 28, 32)            0         \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 11, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,216\n",
      "Trainable params: 9,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Output shape: (None, 11, 32)\n"
     ]
    }
   ],
   "source": [
    "fc_local_model, FC_LOCAL_OUT_SHAPE = create_fc_local()\n",
    "fc_local_model.summary()\n",
    "print(\"\\nOutput shape:\", FC_LOCAL_OUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f1174ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1126"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(11*32)+(3*256)+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5589f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_layer(in_shape=(1137,)):\n",
    "    # fully connected layers that combine local + global and does binary classification\n",
    "\n",
    "    # input shape is flattened fc_local + fc_global + extra star parameters length\n",
    "    '''\n",
    "    input_length =  FC_LOCAL_OUT_SHAPE[1]  * FC_LOCAL_OUT_SHAPE[2]\n",
    "    input_length += FC_GLOBAL_OUT_SHAPE[1] * FC_GLOBAL_OUT_SHAPE[2]\n",
    "    input_length += X_STARPARS_SHAPE[0]\n",
    "    print(\"Input length:\", input_length)\n",
    "    in_layer = Input(shape=(input_length,))\n",
    "    '''\n",
    "    in_layer = Input(shape=in_shape)\n",
    "    fc = Dense(512, activation='relu')(in_layer)\n",
    "    fc = Dense(512, activation='relu')(fc)\n",
    "    fc = Dense(512, activation='relu')(fc)\n",
    "    out_layer = Dense(1, activation='sigmoid')(fc)\n",
    "\n",
    "    model = Model(in_layer, out_layer, name='final_layer_classifier')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d475138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"final_layer_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1137)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               582656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,108,481\n",
      "Trainable params: 1,108,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_layer_model = create_final_layer()\n",
    "final_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90183e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtranetModelCopy():\n",
    "    '''\n",
    "    Extranet Model\n",
    "    INPUT: \n",
    "        x_local\n",
    "        x_global\n",
    "        x_local_cen\n",
    "        x_global_cen\n",
    "        x_star\n",
    "    OUTPUT:\n",
    "        model used for binary classification\n",
    "    '''\n",
    "    print(\"Creating Extranet model\")\n",
    "    # read inputs to the model with given shapes\n",
    "    x_local = Input(shape=X_LOCAL_SHAPE)\n",
    "    x_local_cen = Input(shape=X_LOCAL_CEN_SHAPE)\n",
    "\n",
    "    x_global = Input(shape=X_GLOBAL_SHAPE)\n",
    "    x_global_cen = Input(shape=X_GLOBAL_CEN_SHAPE)\n",
    "\n",
    "    x_star = Input(shape=X_STARPARS_SHAPE)\n",
    "\n",
    "    # concatenate inputs respectively\n",
    "    x_local_all = Concatenate(axis=1)([x_local, x_local_cen]) # these have to be concatenated to shape (X, 2)\n",
    "    x_global_all = Concatenate(axis=1)([x_global, x_global_cen])\n",
    "    \n",
    "    #checking the shape after concat\n",
    "    print(\"x_local_all.shape\", x_local_all.shape)\n",
    "    print(\"x_global_all.shape\", x_global_all.shape)\n",
    "\n",
    "    # reshape the concatenated inputs - **unsure if this reshapes correctly along axis**\n",
    "    x_local_all = Reshape(target_shape=(x_local_all.shape[1]//2, 2))(x_local_all)\n",
    "    x_global_all = Reshape(target_shape=(x_global_all.shape[1]//2, 2))(x_global_all)\n",
    "\n",
    "    #checking the shape after concat\n",
    "    print(\"\\nShape after reshape\")\n",
    "    print(\"x_local_all.shape\", x_local_all.shape)\n",
    "    print(\"x_global_all.shape\", x_global_all.shape)\n",
    "\n",
    "    # call corresponding models\n",
    "    fc_global, _ = create_fc_global()\n",
    "    fc_local, _ = create_fc_local()\n",
    "    \n",
    "    # get outputs from feeing inputs to the models\n",
    "    out_global = fc_global(x_global_all)\n",
    "    out_local = fc_local(x_local_all)\n",
    "\n",
    "    print(\"\\nShape after model outputs\")\n",
    "    print(\"out_global.shape\", out_global.shape)\n",
    "    print(\"out_local.shape\", out_local.shape)\n",
    "    \n",
    "    # do global pooling\n",
    "    '''\n",
    "    out_global = GlobalMaxPool1D() (out_global)\n",
    "    out_local = GlobalMaxPool1D()(out_local)\n",
    "    print(\"local shape after global pooling:\", out_global.shape)\n",
    "    '''\n",
    "    # skipping global pooling bc the dimensionality reduction doesnt make sense to me now\n",
    "\n",
    "    # flatten the outputs\n",
    "    out_global = Flatten()(out_global)\n",
    "    out_local = Flatten()(out_local)\n",
    "\n",
    "    print(\"\\nShape after flatten outputs\")\n",
    "    print(\"out_global.shape\", out_global.shape)\n",
    "    print(\"out_local.shape\", out_local.shape)\n",
    "\n",
    "    # concatenate local, global and stellar params\n",
    "    out = Concatenate()([out_global, out_local, x_star])\n",
    "\n",
    "    print(\"\\nConcatenated out shape:\", out.shape)\n",
    "    print(\"(Should be\", out_global.shape[1], \"global +\", out_local.shape[1], \"local +\", x_star.shape[1], 'stellar params)')\n",
    "    # pass the flattened length to the input shape\n",
    "    final_layer = create_final_layer(in_shape=(out.shape[1],)) # should be 16586\n",
    "\n",
    "    out = final_layer(out)\n",
    "\n",
    "    print(\"\\nShape of output after final layer:\", out.shape)\n",
    "\n",
    "    model = Model([x_local, x_global, x_local_cen, x_global_cen, x_star], out, name='Extranet_model')\n",
    "    opt = Adam(learning_rate=R_LEARN)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "515b0eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Extranet model\n",
      "x_local_all.shape (None, 122)\n",
      "x_global_all.shape (None, 402)\n",
      "\n",
      "Shape after reshape\n",
      "x_local_all.shape (None, 61, 2)\n",
      "x_global_all.shape (None, 201, 2)\n",
      "\n",
      "Shape after model outputs\n",
      "out_global.shape (None, 3, 256)\n",
      "out_local.shape (None, 11, 32)\n",
      "\n",
      "Shape after flatten outputs\n",
      "out_global.shape (None, 768)\n",
      "out_local.shape (None, 352)\n",
      "\n",
      "Concatenated out shape: (None, 1137)\n",
      "(Should be 768 global + 352 local + 17 stellar params)\n",
      "\n",
      "Shape of output after final layer: (None, 1)\n",
      "Model: \"Extranet_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 201)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 201)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 61)]         0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 61)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 402)          0           ['input_6[0][0]',                \n",
      "                                                                  'input_7[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 122)          0           ['input_4[0][0]',                \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 201, 2)       0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 61, 2)        0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " fully_connected_global (Functi  (None, 3, 256)      655232      ['reshape_1[0][0]']              \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " fully_connected_local (Functio  (None, 11, 32)      9216        ['reshape[0][0]']                \n",
      " nal)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 768)          0           ['fully_connected_global[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 352)          0           ['fully_connected_local[0][0]']  \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 17)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 1137)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      " final_layer_classifier (Functi  (None, 1)           1108481     ['concatenate_2[0][0]']          \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,772,929\n",
      "Trainable params: 1,772,929\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "extranet = ExtranetModelCopy()\n",
    "\n",
    "extranet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe130fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(extranet, to_file='extranet.jpg', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1e1476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected global network used for ExtranetXS\n",
    "def create_fc_global():\n",
    "    in_layer = Input(shape=(X_GLOBAL_SHAPE[0], 2))\n",
    "    # unclear what the input shape should be\n",
    "    # extranet has an input of 2 and concatenate the 2 inputs along dim 1\n",
    "\n",
    "    fc = Conv1D(16, kernel_size=5, strides=1, padding='same')(in_layer)\n",
    "    #fc = BatchNormalization()(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = MaxPool1D(pool_size=2, strides=2)(fc)\n",
    "    #fc = Dropout(.5)(fc)\n",
    "\n",
    "    fc = Conv1D(16, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    fc = Activation('relu')(fc)\n",
    "    fc = MaxPool1D(pool_size=2, strides=2)(fc)\n",
    "    #fc = Dropout(.5)(fc)\n",
    "\n",
    "    fc = Conv1D(32, kernel_size=5, strides=1, padding='same')(fc)\n",
    "    #fc = BatchNormalization()(fc)\n",
    "    out_layer = Activation('relu')(fc)\n",
    "    \n",
    "    # save the shape\n",
    "    FC_GLOBAL_OUT_SHAPE = out_layer.shape\n",
    "\n",
    "    model = Model(inputs=in_layer, outputs=out_layer, name='fully_connected_global_xs')\n",
    "    return model, FC_GLOBAL_OUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f087204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Determine input shape!!\n",
    "\n",
    "def create_final_layer(in_shape=(81,)):\n",
    "    # fully connected layers that combine local + global and does binary classification\n",
    "\n",
    "    # input shape is flattened fc_local + fc_global + extra star parameters length\n",
    "    in_layer = Input(shape=in_shape)\n",
    "    fc = Dense(81, activation='relu')(in_layer)\n",
    "    #fc = Dropout(.25)(fc)\n",
    "    #fc = Dense(16, activation='relu')(fc)\n",
    "    #fc = Dropout(.1)(fc)\n",
    "    out_layer = Dense(1, activation='sigmoid')(fc)\n",
    "\n",
    "    model = Model(in_layer, out_layer, name='final_layer_classifier_xs')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "934c4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtranetXSModelCopy():\n",
    "    '''\n",
    "    Extranet Model\n",
    "    INPUT: \n",
    "        x_local\n",
    "        x_global\n",
    "        x_local_cen\n",
    "        x_global_cen\n",
    "        x_star\n",
    "    OUTPUT:\n",
    "        model used for binary classification\n",
    "    '''\n",
    "    print(\"Creating Extranet model\")\n",
    "    # read inputs to the model with given shapes\n",
    "    x_local = Input(shape=X_LOCAL_SHAPE)\n",
    "    x_local_cen = Input(shape=X_LOCAL_CEN_SHAPE)\n",
    "\n",
    "    x_global = Input(shape=X_GLOBAL_SHAPE)\n",
    "    x_global_cen = Input(shape=X_GLOBAL_CEN_SHAPE)\n",
    "\n",
    "    x_star = Input(shape=X_STARPARS_SHAPE)\n",
    "\n",
    "    # concatenate inputs respectively\n",
    "    x_local_all = Concatenate(axis=1)([x_local, x_local_cen]) # these have to be concatenated to shape (X, 2)\n",
    "    x_global_all = Concatenate(axis=1)([x_global, x_global_cen])\n",
    "    \n",
    "    #checking the shape after concat\n",
    "    print(\"x_local_all.shape\", x_local_all.shape)\n",
    "    print(\"x_global_all.shape\", x_global_all.shape)\n",
    "\n",
    "    # reshape the concatenated inputs - **unsure if this reshapes correctly along axis**\n",
    "    x_local_all = Reshape(target_shape=(x_local_all.shape[1]//2, 2))(x_local_all)\n",
    "    x_global_all = Reshape(target_shape=(x_global_all.shape[1]//2, 2))(x_global_all)\n",
    "\n",
    "    #checking the shape after concat\n",
    "    print(\"\\nShape after reshape\")\n",
    "    print(\"x_local_all.shape\", x_local_all.shape)\n",
    "    print(\"x_global_all.shape\", x_global_all.shape)\n",
    "\n",
    "    # call corresponding models\n",
    "    fc_global, _ = create_fc_global()\n",
    "    fc_local, _ = create_fc_local()\n",
    "    \n",
    "    # get outputs from feeing inputs to the models\n",
    "    out_global = fc_global(x_global_all)\n",
    "    out_local = fc_local(x_local_all)\n",
    "\n",
    "    print(\"\\nShape after model outputs\")\n",
    "    print(\"out_global.shape\", out_global.shape)\n",
    "    print(\"out_local.shape\", out_local.shape)\n",
    "    \n",
    "    # do global pooling\n",
    "    out_global = GlobalMaxPool1D() (out_global)\n",
    "    out_local = GlobalMaxPool1D()(out_local)\n",
    "    print(\"out global shape after global pooling:\", out_global.shape)\n",
    "    print(\"out local shape after global pooling:\", out_local.shape)\n",
    "    # skipping global pooling bc the dimensionality reduction doesnt make sense to me now\n",
    "\n",
    "    # flatten the outputs\n",
    "    out_global = Flatten()(out_global)\n",
    "    out_local = Flatten()(out_local)\n",
    "\n",
    "    print(\"\\nShape after flatten outputs\")\n",
    "    print(\"out_global.shape\", out_global.shape)\n",
    "    print(\"out_local.shape\", out_local.shape)\n",
    "\n",
    "    # concatenate local, global and stellar params\n",
    "    out = Concatenate()([out_global, out_local, x_star])\n",
    "\n",
    "    print(\"\\nConcatenated out shape:\", out.shape)\n",
    "    print(\"(Should be\", out_global.shape[1], \"global +\", out_local.shape[1], \"local +\", x_star.shape[1], 'stellar params)')\n",
    "    # pass the flattened length to the input shape\n",
    "    final_layer = create_final_layer(in_shape=(out.shape[1],)) # should be 16586\n",
    "\n",
    "    out = final_layer(out)\n",
    "\n",
    "    print(\"\\nShape of output after final layer:\", out.shape)\n",
    "\n",
    "    model = Model([x_local, x_global, x_local_cen, x_global_cen, x_star], out, name='ExtranetXS_model')\n",
    "    opt = Adam(learning_rate=R_LEARN)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "218b17df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Extranet model\n",
      "x_local_all.shape (None, 122)\n",
      "x_global_all.shape (None, 402)\n",
      "\n",
      "Shape after reshape\n",
      "x_local_all.shape (None, 61, 2)\n",
      "x_global_all.shape (None, 201, 2)\n",
      "\n",
      "Shape after model outputs\n",
      "out_global.shape (None, 50, 32)\n",
      "out_local.shape (None, 11, 32)\n",
      "out global shape after global pooling: (None, 32)\n",
      "out local shape after global pooling: (None, 32)\n",
      "\n",
      "Shape after flatten outputs\n",
      "out_global.shape (None, 32)\n",
      "out_local.shape (None, 32)\n",
      "\n",
      "Concatenated out shape: (None, 81)\n",
      "(Should be 32 global + 32 local + 17 stellar params)\n",
      "\n",
      "Shape of output after final layer: (None, 1)\n",
      "Model: \"ExtranetXS_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 201)]        0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 201)]        0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 61)]         0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 61)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 402)          0           ['input_22[0][0]',               \n",
      "                                                                  'input_23[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 122)          0           ['input_20[0][0]',               \n",
      "                                                                  'input_21[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 201, 2)       0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 61, 2)        0           ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " fully_connected_global_xs (Fun  (None, 50, 32)      4064        ['reshape_5[0][0]']              \n",
      " ctional)                                                                                         \n",
      "                                                                                                  \n",
      " fully_connected_local (Functio  (None, 11, 32)      9216        ['reshape_4[0][0]']              \n",
      " nal)                                                                                             \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 32)          0           ['fully_connected_global_xs[0][0]\n",
      " MaxPooling1D)                                                   ']                               \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 32)          0           ['fully_connected_local[0][0]']  \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 32)           0           ['global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 32)           0           ['global_max_pooling1d_3[0][0]'] \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 17)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 81)           0           ['flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'input_24[0][0]']               \n",
      "                                                                                                  \n",
      " final_layer_classifier_xs (Fun  (None, 1)           1329        ['concatenate_8[0][0]']          \n",
      " ctional)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,609\n",
      "Trainable params: 14,609\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "extranetxs = ExtranetXSModelCopy()\n",
    "extranetxs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e99f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5d11a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = [np.array([x[0] for x in x2_train]), \n",
    "              np.array([x[1] for x in x2_train]), \n",
    "              np.array([x[2] for x in x2_train]), \n",
    "              np.array([x[3] for x in x2_train]), \n",
    "              np.array([x[4] for x in x2_train])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e1ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, feature in enumerate(x_features):\n",
    "    for j, feature_set in enumerate(feature):\n",
    "        if np.any(np.isnan(feature_set)):\n",
    "            print(feature[j][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68cca95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)\n",
    "np.any(np.isnan(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68d331ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.2161 - val_loss: 0.9726\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.2110 - val_loss: 0.9296\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.2071 - val_loss: 0.8949\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.2041 - val_loss: 0.8671\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.2016 - val_loss: 0.8428\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1997 - val_loss: 0.8238\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1980 - val_loss: 0.8081\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1966 - val_loss: 0.7934\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1955 - val_loss: 0.7825\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1944 - val_loss: 0.7710\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1935 - val_loss: 0.7617\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1927 - val_loss: 0.7530\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1920 - val_loss: 0.7466\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1913 - val_loss: 0.7392\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1907 - val_loss: 0.7348\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1901 - val_loss: 0.7285\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1895 - val_loss: 0.7236\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1890 - val_loss: 0.7192\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1885 - val_loss: 0.7135\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1880 - val_loss: 0.7100\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1875 - val_loss: 0.7051\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1870 - val_loss: 0.7025\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1865 - val_loss: 0.6990\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1860 - val_loss: 0.6955\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1856 - val_loss: 0.6936\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1851 - val_loss: 0.6910\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1846 - val_loss: 0.6885\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1842 - val_loss: 0.6847\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1837 - val_loss: 0.6815\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1832 - val_loss: 0.6790\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1828 - val_loss: 0.6760\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1823 - val_loss: 0.6732\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1819 - val_loss: 0.6708\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1815 - val_loss: 0.6682\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1811 - val_loss: 0.6672\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1807 - val_loss: 0.6644\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1802 - val_loss: 0.6641\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1798 - val_loss: 0.6594\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1794 - val_loss: 0.6570\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1790 - val_loss: 0.6573\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1787 - val_loss: 0.6548\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1783 - val_loss: 0.6536\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1781 - val_loss: 0.6512\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1777 - val_loss: 0.6512\n",
      "Epoch 45/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1775 - val_loss: 0.6487\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1772 - val_loss: 0.6457\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1769 - val_loss: 0.6443\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1766 - val_loss: 0.6443\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1764 - val_loss: 0.6418\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1761 - val_loss: 0.6418\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1759 - val_loss: 0.6408\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1757 - val_loss: 0.6423\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1754 - val_loss: 0.6399\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1752 - val_loss: 0.6361\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1750 - val_loss: 0.6363\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1748 - val_loss: 0.6355\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1745 - val_loss: 0.6363\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1743 - val_loss: 0.6367\n",
      "Epoch 59/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1741 - val_loss: 0.6361\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1739 - val_loss: 0.6334\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1737 - val_loss: 0.6376\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1735 - val_loss: 0.6354\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1733 - val_loss: 0.6360\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1731 - val_loss: 0.6367\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1729 - val_loss: 0.6333\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1727 - val_loss: 0.6328\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1725 - val_loss: 0.6356\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1723 - val_loss: 0.6345\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1721 - val_loss: 0.6347\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1719 - val_loss: 0.6325\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1717 - val_loss: 0.6324\n",
      "Epoch 72/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1716 - val_loss: 0.6311\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1714 - val_loss: 0.6317\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1711 - val_loss: 0.6313\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1709 - val_loss: 0.6322\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1708 - val_loss: 0.6344\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1706 - val_loss: 0.6307\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1704 - val_loss: 0.6264\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1702 - val_loss: 0.6260\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1700 - val_loss: 0.6259\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1698 - val_loss: 0.6280\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1697 - val_loss: 0.6242\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1695 - val_loss: 0.6253\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1693 - val_loss: 0.6226\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1692 - val_loss: 0.6220\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1690 - val_loss: 0.6224\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1688 - val_loss: 0.6193\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1687 - val_loss: 0.6233\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1686 - val_loss: 0.6204\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1683 - val_loss: 0.6211\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1681 - val_loss: 0.6203\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1680 - val_loss: 0.6184\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1678 - val_loss: 0.6177\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1677 - val_loss: 0.6177\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1675 - val_loss: 0.6187\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1674 - val_loss: 0.6142\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1672 - val_loss: 0.6165\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1670 - val_loss: 0.6157\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1669 - val_loss: 0.6188\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1667 - val_loss: 0.6156\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:0.83, 1:0.17}\n",
    "history = extranetxs.fit(x=x_features, y=y_train, validation_split=0.2, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28e3d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = [\n",
    "    np.array([x[0] for x in x2_val]), \n",
    "    np.array([x[1] for x in x2_val]), \n",
    "    np.array([x[2] for x in x2_val]), \n",
    "    np.array([x[3] for x in x2_val]), \n",
    "    np.array([x[4] for x in x2_val])\n",
    "]\n",
    "\n",
    "for i, feature in enumerate(val_features):\n",
    "    if np.any(np.isnan(feature[i])):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "daf4bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "val_features = [\n",
    "    np.array([x[0] for x in x2_val]), \n",
    "    np.array([x[1] for x in x2_val]), \n",
    "    np.array([x[2] for x in x2_val]), \n",
    "    np.array([x[3] for x in x2_val]), \n",
    "    np.array([x[4] for x in x2_val])\n",
    "]\n",
    "val_output = extranetxs.predict(val_features)\n",
    "\n",
    "val_gt = y2_val.astype(None).ravel()\n",
    "val_output = val_output.astype(None).ravel()\n",
    "save(\"./outputs/train_y_output.npy\", val_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a3acc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6158694 , 0.32004273, 0.79221857, 0.3263402 , 0.68549871,\n",
       "       0.67956609, 0.37672141, 0.33248714, 0.53902423, 0.47728008,\n",
       "       0.37497672, 0.52130622, 0.60642874, 0.67957544, 0.63869733,\n",
       "       0.63071597, 0.62321395, 0.464237  , 0.58233899, 0.61538494,\n",
       "       0.55194438, 0.65170532, 0.63255757, 0.53616673, 0.4437454 ,\n",
       "       0.65430319, 0.25823125, 0.40534496, 0.6716035 , 0.44599763,\n",
       "       0.61092532, 0.71989375, 0.63745815, 0.11120214, 0.73936135,\n",
       "       0.73045141, 0.41856456, 0.00107319, 0.6138767 , 0.67745066,\n",
       "       0.47993246, 0.67513043, 0.59079993, 0.73292112, 0.44571263,\n",
       "       0.4331103 , 0.5733012 , 0.69608206, 0.47559574, 0.23935579,\n",
       "       0.47999874, 0.39573774, 0.74032158, 0.46368113, 0.53743827,\n",
       "       0.48184958, 0.61280668, 0.51057684, 0.38391432, 0.55898851,\n",
       "       0.58916861, 0.5909971 , 0.64749271, 0.50032729, 0.57368857,\n",
       "       0.36494803, 0.62535173, 0.66186041, 0.50370675, 0.5257836 ,\n",
       "       0.69579017, 0.43006715, 0.49477676, 0.53993595, 0.65322793,\n",
       "       0.73350888, 0.65381354, 0.72129375, 0.55397689, 0.83990973,\n",
       "       0.49823523, 0.17615999, 0.49737105, 0.48971453, 0.39582083,\n",
       "       0.76192266, 0.584135  , 0.50529963, 0.45141414, 0.66362435,\n",
       "       0.32294035, 0.43981177, 0.45022875, 0.69833612, 0.70971924,\n",
       "       0.61275488, 0.81352401, 0.72755116, 0.4919174 , 0.74058938,\n",
       "       0.23928407, 0.75154448, 0.66753644, 0.50957674, 0.50854093,\n",
       "       0.69287229, 0.51362783, 0.45944598, 0.59827495, 0.61599433,\n",
       "       0.57986373, 0.64547414, 0.38318211, 0.18961956, 0.15054253,\n",
       "       0.71744615, 0.4371455 , 0.67512178, 0.24212766, 0.39940709,\n",
       "       0.1348801 , 0.54713398, 0.5073598 , 0.00556433, 0.34806386,\n",
       "       0.46447644, 0.60611433, 0.6375699 , 0.51748657, 0.5718258 ,\n",
       "       0.53568631, 0.78167868, 0.57156414, 0.41332141, 0.44257849,\n",
       "       0.23474896, 0.65353566, 0.59504384, 0.54024249, 0.71815914,\n",
       "       0.78789932, 0.63718551, 0.54192668, 0.29423881, 0.63713485,\n",
       "       0.74747241, 0.39522895, 0.58428735, 0.39347672, 0.43544668,\n",
       "       0.55001014, 0.26376197, 0.65663111, 0.34370479, 0.56398827,\n",
       "       0.47980237, 0.542171  , 0.67216074, 0.53701025, 0.49090537,\n",
       "       0.56047744, 0.68467498, 0.48301923, 0.5158006 , 0.69974852,\n",
       "       0.7439726 , 0.59408152, 0.30570373, 0.69840163, 0.72579068,\n",
       "       0.61052775, 0.64463317, 0.65166271, 0.40096214, 0.29266745,\n",
       "       0.52899665, 0.40209058, 0.45961192, 0.46489576, 0.53396237,\n",
       "       0.28581443, 0.5584107 , 0.46816254, 0.45885924, 0.45530984,\n",
       "       0.67962974, 0.00730874, 0.56695455, 0.42034957, 0.5073598 ,\n",
       "       0.60907751, 0.57077676, 0.5797624 , 0.67128229, 0.26573685,\n",
       "       0.22740161, 0.52494496, 0.55775625, 0.52206486, 0.38082802,\n",
       "       0.66445291, 0.19480856, 0.56063175, 0.49624607, 0.66186011,\n",
       "       0.49065363, 0.60268766, 0.52155095, 0.65764862, 0.55163193,\n",
       "       0.34785393, 0.72074825, 0.71055335, 0.65754211, 0.47206146,\n",
       "       0.54758447, 0.60339588, 0.35674277, 0.33587366, 0.61415625,\n",
       "       0.42288747, 0.59385985, 0.33914691, 0.5931583 , 0.46335617,\n",
       "       0.54250902, 0.67910266, 0.44928077, 0.60103041, 0.41686404,\n",
       "       0.71072388, 0.03016356, 0.30161524, 0.34903872, 0.62027884,\n",
       "       0.58066225, 0.51948297, 0.66618848, 0.66431785, 0.67402124,\n",
       "       0.55475897, 0.62529552, 0.66659105, 0.29112056, 0.58979523,\n",
       "       0.71772844, 0.41339535, 0.56379521, 0.52682   , 0.72310162,\n",
       "       0.35167357, 0.19750215, 0.52377409, 0.44153064, 0.25799352,\n",
       "       0.58230448, 0.62678158, 0.56302774, 0.61691511, 0.60761875,\n",
       "       0.54257119, 0.69766104, 0.56456268, 0.51292753, 0.71894139,\n",
       "       0.46132323, 0.6253432 , 0.53753203, 0.52597505, 0.47502744,\n",
       "       0.57040262, 0.53887868, 0.44372424, 0.50954795, 0.3994793 ,\n",
       "       0.64026642, 0.59402514, 0.66922134, 0.6596911 , 0.68563908,\n",
       "       0.12936586, 0.46039343, 0.52850956, 0.44648397, 0.41112173,\n",
       "       0.33976769, 0.37992811, 0.61363715, 0.69857925, 0.57722926,\n",
       "       0.74827009, 0.60624081, 0.5128485 , 0.69947916, 0.55181903,\n",
       "       0.50873214, 0.19045411, 0.57057935, 0.44568244, 0.5828203 ,\n",
       "       0.64688104, 0.63948584, 0.62109882, 0.53293508, 0.62156135,\n",
       "       0.62350416])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "835d7dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CALCULATING METRICS...\n",
      "\n",
      "   thresh = 0.50, precision = 0.91, recall = 0.70\n",
      "      TN = 34, FP = 18, FN = 75, TP = 179\n",
      "   thresh = 0.60, precision = 0.91, recall = 0.40\n",
      "      TN = 42, FP = 10, FN = 152, TP = 102\n",
      "   thresh = 0.70, precision = 0.97, recall = 0.11\n",
      "      TN = 51, FP = 1, FN = 226, TP = 28\n",
      "   thresh = 0.80, precision = 1.00, recall = 0.01\n",
      "      TN = 52, FP = 0, FN = 252, TP = 2\n",
      "   thresh = 0.90, precision = 0.00, recall = 0.00\n",
      "      TN = 52, FP = 0, FN = 254, TP = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\socia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "########################################\n",
    "####### CALCULATE STATISTICS ###########\n",
    "########################################\n",
    "\n",
    "### setup screen output\n",
    "print(\"\\nCALCULATING METRICS...\\n\")\n",
    "\n",
    "### calculate average precision & precision-recall curves\n",
    "AP = average_precision_score(val_gt, val_output, average=None)\n",
    "# print(\"   average precision = {0:0.4f}\\n\".format(AP))\n",
    "\n",
    "### calculate precision-recall curve\n",
    "P, R, _ = precision_recall_curve(val_gt, val_output)\n",
    "\n",
    "### calculate confusion matrix based on different thresholds\n",
    "thresh = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "prec_thresh, recall_thresh = np.zeros(len(thresh)), np.zeros(len(thresh))\n",
    "\n",
    "for n, nval in enumerate(thresh):\n",
    "    pred_byte = np.zeros(len(val_output))\n",
    "    for i, val in enumerate(val_output):\n",
    "        if val > nval:\n",
    "            pred_byte[i] = 1.0\n",
    "        else:\n",
    "            pred_byte[i] = 0.0\n",
    "    prec_thresh[n] = precision_score(val_gt, pred_byte)\n",
    "    recall_thresh[n] = recall_score(val_gt, pred_byte)\n",
    "    print(\"   thresh = {0:0.2f}, precision = {1:0.2f}, recall = {2:0.2f}\".format(thresh[n], prec_thresh[n], recall_thresh[n]))\n",
    "    tn, fp, fn, tp = confusion_matrix(val_gt, pred_byte).ravel()\n",
    "    print(\"      TN = {0:0}, FP = {1:0}, FN = {2:0}, TP = {3:0}\".format(tn, fp, fn, tp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef581c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20bea559a50>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f92e03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20bec63f400>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(P)), P)\n",
    "plt.plot(np.arange(len(R)), R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d30a78ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20bec6af790>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history.epoch, history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eb3741e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9b37a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b288b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    669\n",
       "1.0    513\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y2_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "092c665c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3240+653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a6208aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "417+71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "93385193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14549180327868852"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "71/488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f89cc09f538455e24f42a9c8f3bfeae5e7a28ad71f39b65c003d38363937261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
